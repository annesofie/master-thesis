\chapter{Discussion}
Micro-tasking is a repetitive method when projects wish to exploit human computation and a huge crowd, through for instance crowdsourcing. However, there is little research, if any, on using the micro-tasking method with interactive maps and geospatial data. In OpenStreetMap, the method has become a dominant method, both in mapping jobs and imports jobs \citep{Erichsen2016}. The methods popularity in OSM confirms the potential of expanding micro-tasking to the geo-community. This thesis test if micro-tasking can or should expand to the mapping community, also outside OSM. The research hypothesis tested is 1) Inexperienced workers cannot solve micro-tasks containing geospatial data as good as experienced workers and 2) The fewer elements in a micro-task, the better the worker solves the task.  

The first research hypothesis is rejected. There was a statistically significant evidence that inexperienced participants finished the tasks faster than experienced participants, this is also shown in figure \ref{fig:meanstdparticipantstime}. The mean time difference was 16 seconds, a statistically significant, but small difference. This result is surprising. One would expect that experienced participants is familiar with map interaction and visually analyzing geometries in base maps and also interpreting meta information. During the pilot test, the author noticed that the experienced participants used the map aids given in question one more frequently than inexperienced. The map aids were zoom, panning and a layer control to show/hide the building footprints. A possible explanation for the time difference is that the experienced participants spent more time using the map aids provided since they are more familiar with them and know how to use it. 

When analyzing the number of correctly chosen elements, the difference is not statistically significant. Figure \ref{fig:meanstdparticipantscorrect} also show a very similar mean value. Inexperienced participants had a mean of 9.83 correct elements, while experienced had a mean of 9.81 correct elements. One would expect that inexperienced participants had fewer correct elements since they spent less time on the tasks. \cite{Salk2016} results also had minor differenced between the professional and non-professional participants. They concluded that professional background had a limited first-order relationship with task accuracy. The participant's local knowledge had a much more significant impact on the results. It can be argued that the design of the question interfaces, together with the introduction video and training task, was so easy to use that the professional background had no effect on the quality of the task results. One can also suspect the experienced to not follow the instructions video as cautiously as inexperienced. 

When dividing the participants into experienced and inexperienced is was based one the single question "Do you have experience of working with geospatial data?". Other studies asks the participants for background information through a registration procedure. \cite{See2013} and \cite{Salk2016} considered people with a background in remote sensing/spatial science as experts, and people who were new to the discipline or had a self-declared limited background as non-experts. In this study the participants are self-declared experts / non-experts. It is not possible to validate this information, but when knowing the background of the pilot-test participants they answered yes and no on the experience question as the author anticipated. 

The second research hypothesis is evaluated as rejected. There is very little difference between the three tasks. The statistical analysis, when looking at time to complete each task, concluded that there was no statistical difference between the tasks. Figure \ref{fig:meanstdexperiencedtask123time} show a slightly faster task completion on task A, but not a statistical significant difference according to \textit{Tukey's test}. It can be argued that the participants spent more time on task A in total. Time spent switching to the next element in each question is not added to the time variable. In total the participant probably spent more time in front of the screen doing task A compared to task C. Looking at the quality of the task completion, task A is statistical significant better than the two other tasks according to \textit{Tukey's test}, show in figure \ref{fig:meanstdexperiencedtask123correct}. Using the figure and table \ref{tab:totalcorrect_tasks}, participants had in average one more correct element in task A compared to the two other tasks. Participants did worst on task C, but the difference is small. Task A were given an average difficulty rating of 2.11, task B of 2.14 and task C of 2.5  where a score of 1 was described as easy and 5 hard (task 4 1.79). Experienced participants gave a lower difficulty score than inexperienced on all three tasks (in average a 0.20 lower difficulty score).

There is potentially a lot of work creating micro-tasks. The large task needs to be appropriately broken down to micro-tasks that are easy, enjoyable, and fast to solve. This breakdown requires design skills and proper tutorials and examples for new workers \citep{Schulze2012}. Humans make mistakes and these errors need to be detected. Quality mechanisms also needs to be determined, so that the output data is of high quality, without errors. To avoid putting too much emphasis on the preparations, guidelines can help to best utilize the preparation resources. The results from this thesis can be used as a guidance. 

The overall task result quality was good. One can doubt the difficulty of the questions given in the tasks. The design of the questions was maybe not realistic enough. 

The executed analyses in chapter four focus on the total time and correct elements in each task instead of separating in question one and two 