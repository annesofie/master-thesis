\chapter{Discussion}
Micro-tasking is an recurring method used to exploit human computation and a huge crowd, through for instance crowdsourcing. However, using the micro-tasking method using interactive maps and geospatial data has been little studied. In OpenStreetMap the method has become a dominant method, both in mapping jobs and imports jobs. This shows the potential of expanding micro-tasking to the geo-community. This thesis will test if micro-tasking can or should expand to the mapping community, also outside OSM. The research hypothesis tested is 1) Inexperienced workers cannot solve micro-tasks containing geospatial data as good as experienced workers and 2) The fewer elements in a micro-task, the better the worker solves the task. 

The first research hypothesis is rejected. There was a statistically significant evidence that inexperienced participants finished the tasks faster than experienced participants, this is also shown in figure \ref{fig:meanstdparticipantstime}. The mean time difference was 16 seconds, a statistical significant, but small difference. This result is surprising. One would think that experienced participants to be familiar with map interaction and visually analysing information in base maps. When analysing the number of correctly chosen elements, the difference is not statistically significant. Figure \ref{fig:meanstdparticipantscorrect} also show a very similar mean value. Inexperienced participants had an mean of 9.83 correct elements, while experienced had a mean of 9.81 correct elements. \cite{Salk2016} results also had minor differenced between the professional and non-professional participants. They concluded that professional background had little first-order relationship with task accuracy. The participants local knowledge had a much more significant impact on the results. When dividing the participants into experienced and inexperienced is was done one the single question "Do you have experience of working with geospatial data?". 

Task 1 had an average difficulty rating of 2.11, task 2 of 2.14 and task 3 of 2.5 (task 4 1.79). Experienced participants gave a lower difficulty score than inexperienced on all three tasks (in average a 0.20 lower difficulty score).

There is potentially a lot of work creating micro-tasks. The tasks needs to be appropriately broken down to micro-tasks that are easy, enjoyable, and fast. This breakdown requires design skills and proper tutorials and examples for new workers \citep{Schulze2012}. Humans make mistakes and these errors need to be detected. A quality mechanism needs to be present, so that the output data is of high quality, without errors. To avoid putting too much emphasis on the preparations guidelines can help to best utilize the preparation resources. The statistical analysis on the gathered data show very little difference among task A, B, and C. When looking at time to complete each task, figure \ref{fig:meanstdexperiencedtask123time} show a slightly faster task completion on task A, but not a statistical significant difference according to \textit{Tukey's test}. Looking at the quality of the task completion, task A is statistical significant better than the two other tasks according to \textit{Tukey's test}, show in figure \ref{fig:meanstdexperiencedtask123correct}. 

The overall task result quality was good. One can doubt the difficulty of the questions given in the tasks. The design of the questions was maybe not realistic enough. 

The executed analyses in chapter four focus on the total time and correct elements in each task instead of separating in question one and two 