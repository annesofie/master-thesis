\chapter{Discussion}
Micro-tasking is often used  in projects that wish to exploit human computation and a huge crowd, through for instance crowdsourcing. However, there is little research, if any, on using the micro-tasking method with interactive maps and geospatial data. In OpenStreetMap, the method has become a dominant method, both in mapping jobs and imports jobs \citep{Erichsen2016}. The methods popularity in OSM confirms the potential of expanding micro-tasking to the geo-community. This thesis test if micro-tasking can or should expand to the mapping community, also outside OSM. The research hypothesis tested is 1) Inexperienced workers cannot solve micro-tasks containing geospatial data as good as experienced workers and 2) The fewer elements in a micro-task, the better the worker solves the task.  

We evaluate the first research hypothesis as rejected. There was a statistically significant evidence that inexperienced participants finished the tasks faster than experienced participants, this is also shown in figure \ref{fig:meanstdparticipantstime}. The mean time difference was 16 seconds, a statistically significant, but a relatively small difference. One would expect that experienced participants is familiar with map interaction and visually analyzing geometries in base maps and also interpreting meta information. During the pilot test, the author noticed that the experienced participants used the map aids given in question one more frequently than inexperienced. The map aids were zoom, panning and a layer control to show/hide the building footprints. A possible explanation for the time difference is that the experienced participants spent more time using the map aids provided since they are more familiar with them and knew how to use them.

When analyzing the number of correctly chosen elements between experienced and inexperienced, the difference is not statistically significant. Figure \ref{fig:meanstdparticipantscorrect} show a very similar mean value in the number of correct elements. Inexperienced participants had a mean of 9.83 correct elements, while experienced had a mean of 9.81 correct elements. One would expect that inexperienced participants had fewer correct elements since they spent less time on the tasks. \cite{Salk2016} results also had minor differenced between the professional and non-professional participants. They concluded that professional background had a limited first-order relationship with task accuracy. It can be argued that the design of the question interfaces, together with the introduction video and training task, was so easy to use that the professional background had no effect on the quality of the task results. \cite{See2013} concluded that with proper targeted training material the differences between experts and non-experts could decrease. One can also suspect the experienced not to follow the instructions video as carefully as inexperienced. 

The splitting of participants into experienced and inexperienced, was based on the question "Do you have experience of working with geospatial data?". Other studies ask the participants for background information through a registration procedure. \cite{See2013} and \cite{Salk2016} considered people with a background in remote sensing/spatial science as experts, and people who were new to the discipline or had a self-declared limited background as non-experts. In this study, the participants are self-declared experts / non-experts. It is not possible to validate this information. In the pilot-test we knew the background of the participants, they answered yes and no on the experience question as the author anticipated. 

In the second research hypothesis, it is not as easy to come to a conclusion. There are minor differences between the three tasks. The statistical analysis concluded that there was no statistical difference between the tasks when considering the time variable. Time spent completing each task was approximately the same. Figure \ref{fig:meanstdexperiencedtask123time} show a slightly faster task completion on task A, but not a statistically significant difference according to \textit{Tukey's test}. 

The time variable does not reflect how much time the participant spent in front of the screen from task start to task end. It reflects how much time passed when solving the two questions. It can be argued that the participants spend more time on task A in total. Time spent switching to the next element in each question is not added to the time variable. In total the participant probably spent more time in front of the screen doing task A compared to task C. In task C all six elements are present, so the participants did not have to wait for the web application to switch to the next task element. In the Los Angeles building import, they used an approach wich combines task A and task C. The buildings were imported one by one, but in their solution, all buildings covering a selected area was visible on the map, but the map window highlighted one and one building. This approach eliminates the time spent fetching and switching to the next building footprint on the map.  

Looking at the quality of the task results, task A is statistical significant better than the two other tasks according to \textit{Tukey's test}, shown in figure \ref{fig:meanstdexperiencedtask123correct}. Using the figure and table \ref{tab:totalcorrect_tasks}, participants had in average one more correct element in task A compared to the two other tasks. Participants did worst on task C, but the difference is small. Figure \ref{fig:splittedbymeanage} show mean task quality for the three different tasks, divided by the participant's average age (31.5 years). The youngest participants did better on all three tasks. Task A has the highest task quality in both groups. Looking at the participants above mean age, \ref{fig:olderthan31correct}, task C has poorer quality. Older participants struggle more on micro-tasks with six elements than three and one elements. Inexperienced participants had an average better quality on task C than the older, but this statement is not statistically tested. 

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../../thesis-statisticmethods/statistic_analysis/figures/youngerthan_31_correct}
		\caption{Younger than 31.5 years}
		\label{fig:youngerthan31correct}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../../thesis-statisticmethods/statistic_analysis/figures/olderthan_31_correct}
		\caption{Older than 31.5 years}
		\label{fig:olderthan31correct}
	\end{subfigure}
	\caption{Mean (green dot) and standard deviation (blue line) - correct elements per task, participants divided by mean age}
	\label{fig:splittedbymeanage}
\end{figure}

The results from this thesis can be used as guidance on how to break down the large geospatial task. If quality is the key factor, the large task must be divided into one and one element. The downside of this approach is the time spent on not task specific operations. If the worker needs to click and wait for the next element to load, the workers will spend unnecessary time. If quality mechanisms already are implemented it is possible to break the task into larger chunks (containing, i.e., three or six elements). The number of mistakes will probably increase with the number of elements in the chunk, but if the quality mechanisms are well developed, the errors should be discovered. Benefits of dividing each task into chunks, containing more than one element, is that the worker will most likely spend less time in total doing the micro-tasks. We would not recommend chunks containing more than six elements unless one uses well-developed quality mechanisms. 




