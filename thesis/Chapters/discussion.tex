\chapter{Discussion}
As shown in this thesis, micro-tasking is used in projects that wish to exploit human computation and a huge crowd, through for instance crowdsourcing. This thesis study if micro-tasking can or should expand to involve interactive maps and geospatial data. In OpenStreetMap, the micro-tasking method has become a dominant method, both in mapping jobs and imports jobs \citep{Erichsen2016}. The methods popularity in OSM confirms the potential of expanding micro-tasking to the geo-community. The research questions tested is: 1) Is it possible to give micro-tasks containing geospatial data to inexperienced workers? 2) Will the quality of the solved task increase with fewer elements present in each micro-task? 3) What are the number of elements optimal within a micro-task to get it completed as quickly as possible?

Our results have found support for the possibility of giving geospatial micro-tasks to all individuals, independent of background. There was statistically significant evidence that inexperienced participants finished the tasks faster than experienced participants, this is also shown in figure \ref{fig:meanstdparticipantstime}. The mean time difference was 16 seconds, a statistically significant, but a relatively small difference. One would expect that experienced participants finished the tasks faster since they are familiar with map interaction and interpreting base maps and meta information. During the pilot test, the author noticed that the experienced participants used the map aids given in question one more frequently than inexperienced. The map aids were zoom, panning and a layer control to show/hide the building footprints. A possible explanation for the time difference is that the experienced participants spent more time using the map aids provided since they are more familiar with them and knew how to use them. 

When analyzing the number of correctly chosen elements between experienced and inexperienced, the difference is not statistically significant. Figure \ref{fig:meanstdparticipantscorrect} show a very similar mean value in the number of correct elements. Inexperienced participants had a mean of 9.83 correct elements, while experienced had a mean of 9.81 correct elements. One would expect that inexperienced participants had fewer correct elements since they spent less time on the tasks. \cite{Salk2016} results also had minor differenced between the professional and non-professional participants. They concluded that professional background had a limited first-order relationship with task accuracy. It can be argued that the design of the question interfaces, together with the introduction video and training task, was so easy to use that the professional background had no effect on the quality of the task results. \cite{See2013} concluded that with proper targeted training material the differences between experts and non-experts could decrease. One can also suspect the experienced not to follow the instructions video as carefully as inexperienced since they already knew how to use interactive maps. This statement cannot be verified.

The splitting of participants into experienced and inexperienced, was based on the question "Do you have experience of working with geospatial data?". Other studies ask the participants for background information through a registration procedure. \cite{See2013} and \cite{Salk2016} considered people with a background in remote sensing/spatial science as experts, and people who were new to the discipline or had a self-declared limited background as non-experts. In this study, the participants are self-declared experts / non-experts. It is not possible to validate this information. In the pilot-test we knew the background of the participants, they answered yes and no on the experience question as the author anticipated. 

There were minor differences between the three tasks. Results found no statistical support for faster completion time with fewer elements. The statistical analysis concluded that there was no statistical difference between the tasks when considering the time variable. Time spent completing each task was approximately the same. Figure \ref{fig:meanstdexperiencedtask123time} show a slightly faster task completion on task A, but not a statistically significant difference according to \textit{Tukey's test}. Experienced participants finished task B fastest (figure \ref{fig:meanstdexperiencedtask123time}) and inexperienced finished task A fastest (figure \ref{fig:meanstdinexperiencedtask123time}) but this is not statistically significant. 

The time variable does not reflect how much time the participant spent in front of the screen from task start to task end. It reflects how much time passed when solving the two questions. It can be argued that the participants spend more time on task A in total. Time spent switching to the next question and fetching the next task element is not added to the time variable. In total the participant probably spent more time in front of the screen doing task A compared to task C. In task C all six task elements are present, so the participants did not have to wait for the web application to fetch the next task element. In the Los Angeles building import, they used an approach which combines task A and task C. The buildings were imported one by one, but in their solution, all buildings covering a selected area was visible on the map, but the map window highlighted one and one building. This approach eliminates the time spent fetching and switching to the next building footprint on the map.  

Looking at the quality of the task results, task A is statistical significant better than the two other tasks according to \textit{Tukey's test}. Our results have found support for the quality to increase with fewer elements present in the task, also shown in figure \ref{fig:meanstdtask123pngcorrect}. Using the figure and table \ref{tab:totalcorrect_tasks}, participants had in average one more correct element in task A compared to the two other tasks. Participants did worst on task C, but the difference is small. Experienced participants got better results on task A, and this was also statistically significant. Inexperienced participants also got the best results on task A, but this is not statistically significant. The quality of task results was more even in the three tasks for inexperienced than experienced participants. This can be seen in figure \ref{fig:meanstdexperiencedtask123correct} and \ref{fig:meanstdinexperiencedtask123correct}. 

Figure \ref{fig:splittedbymeanage} show mean task quality for the three different tasks, divided by the participant's average age (31.5 years). The youngest participants did better on all three tasks. Task A has the highest task quality in both groups. Looking at the participants above mean age, \ref{fig:olderthan31correct}, task C has poorer quality. Older participants struggle more on micro-tasks with six elements than three and one elements. Inexperienced participants had an average better quality on task C than the older, but this statement is not statistically tested. 

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../../thesis-statisticmethods/statistic_analysis/figures/youngerthan_31_correct}
		\caption{Younger than 31.5 years}
		\label{fig:youngerthan31correct}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../../thesis-statisticmethods/statistic_analysis/figures/olderthan_31_correct}
		\caption{Older than 31.5 years}
		\label{fig:olderthan31correct}
	\end{subfigure}
	\caption{Mean (green dot) and standard deviation (blue line) - correct elements per task, participants divided by mean age}
	\label{fig:splittedbymeanage}
\end{figure}

Task A had an average difficulty score of 2.11, task B of 2.14 and task C of 2.5. Experienced participants gave a lower difficulty score than inexperienced on all three tasks (in average a 0.20 lower difficulty score). Participants aged below mean age gave a difficulty score of 2.30 on task A, 2.20 on task B and 2.57 on task C. The higher half of the participants rated the difficulty lower. They gave task A a difficulty score of 1.81, task B 2.05 and task C 2.39. All scores are surprisingly low. Score one was explained as easy, and a score five was described as hard. 

The results from this thesis can be used as guidance on how to break down the large geospatial task. If quality is the key factor, the large task must be divided into one and one element. The downside of this approach is the time spent on not task specific operations. If the worker needs to click and wait for the next element to load, the workers will spend unnecessary time. If quality mechanisms already are implemented it is possible to break the task into larger chunks (containing, i.e., three or six elements). The number of mistakes will probably increase with the number of elements in the chunk, but if the quality mechanisms are well developed, the errors should be discovered. Benefits of dividing each task into chunks, containing more than one element, is that the worker will most likely spend less time in total doing the micro-tasks. We would not recommend chunks containing more than six elements unless one uses well-developed quality mechanisms. 




