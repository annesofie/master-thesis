\chapter{Discussion}
Micro-tasking is a repetitive method when projects wish to exploit human computation and a huge crowd, through for instance crowdsourcing. However, there is little research, if any, on using the micro-tasking method with interactive maps and geospatial data. In OpenStreetMap, the method has become a dominant method, both in mapping jobs and imports jobs \citep{Erichsen2016}. The methods popularity in OSM confirms the potential of expanding micro-tasking to the geo-community. This thesis test if micro-tasking can or should expand to the mapping community, also outside OSM. The research hypothesis tested is 1) Inexperienced workers cannot solve micro-tasks containing geospatial data as good as experienced workers and 2) The fewer elements in a micro-task, the better the worker solves the task.  

The first research hypothesis is rejected. There was a statistically significant evidence that inexperienced participants finished the tasks faster than experienced participants, this is also shown in figure \ref{fig:meanstdparticipantstime}. The mean time difference was 16 seconds, a statistically significant, but small difference. This result is surprising. One would expect that experienced participants is familiar with map interaction and visually analyzing geometries in base maps and also interpreting meta information. During the pilot test, the author noticed that the experienced participants used the map aids given in question one more frequently than inexperienced. The map aids were zoom, panning and a layer control to show/hide the building footprints. A possible explanation for the time difference is that the experienced participants spent more time using the map aids provided since they are more familiar with them and knew how to use it. 

When analyzing the number of correctly chosen elements, the difference is not statistically significant. Figure \ref{fig:meanstdparticipantscorrect} also show a very similar mean value. Inexperienced participants had a mean of 9.83 correct elements, while experienced had a mean of 9.81 correct elements. One would expect that inexperienced participants had fewer correct elements since they spent less time on the tasks. \cite{Salk2016} results also had minor differenced between the professional and non-professional participants. They concluded that professional background had a limited first-order relationship with task accuracy. The participant's local knowledge had a much more significant impact on the results. It can be argued that the design of the question interfaces, together with the introduction video and training task, was so easy to use that the professional background had no effect on the quality of the task results. One can also suspect the experienced to not follow the instructions video as cautiously as inexperienced. 

When dividing the participants into experienced and inexperienced is was based on the question "Do you have experience of working with geospatial data?". Other studies asks the participants for background information through a registration procedure. \cite{See2013} and \cite{Salk2016} considered people with a background in remote sensing/spatial science as experts, and people who were new to the discipline or had a self-declared limited background as non-experts. In this study the participants are self-declared experts / non-experts. It is not possible to validate this information, but when knowing the background of the pilot-test participants they answered yes and no on the experience question as the author anticipated. 

The second research hypothesis is more difficult to answer. There is minor differences between the three tasks. Looking at the time variable, the statistical analysis concluded that there was no statistical difference between the tasks. Time spent completing each task was approximately the same. Figure \ref{fig:meanstdexperiencedtask123time} show a slightly faster task completion on task A, but not a statistical significant difference according to \textit{Tukey's test}. 

The time variable do not reflect how much time the participant spent in front of the screen, it reflects how much time elapsed to solve the two questions. It can be argued that the participants spent more time on task A in total. Time spent switching to the next element in each question is not added to the time variable. In total the participant probably spent more time in front of the screen doing task A compared to task C. In task C all six elements are present, so the participants did not have to wait for the web application to switch to the next task element. In the Los Angeles building import the buildings were imported one and one. Here all buildings covering a selected area was visible on the map, but the map window highlighted one and one building. This approach is a combination of task A and C, and time spent switching building footprint is removed. 

Looking at the quality of the task results, task A is statistical significant better than the two other tasks according to \textit{Tukey's test}, show in figure \ref{fig:meanstdexperiencedtask123correct}. Using the figure and table \ref{tab:totalcorrect_tasks}, participants had in average one more correct element in task A compared to the two other tasks. Participants did worst on task C, but the difference is small. Figure \ref{fig:splittedbymeanage} show mean task quality for the three different tasks, divided by mean age (31.5 years). The youngest participants did better on all three tasks. Task A has the highest quality in both groups. Looking at the participants above mean age (31.5 years), \ref{fig:olderthan31correct}, task C has worst quality. Older participants struggle more on micro-tasks with six elements than three and one element. Inexperienced participants had an average better quality on task C than the older, but this statement was not statistically analysed. 

\begin{figure}[H]
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../../thesis-statisticmethods/statistic_analysis/figures/youngerthan_31_correct}
		\caption{Younger than 31.5 years}
		\label{fig:youngerthan31correct}
	\end{subfigure}
	\begin{subfigure}[b]{0.4\textwidth}
		\centering
		\includegraphics[width=\linewidth]{../../thesis-statisticmethods/statistic_analysis/figures/olderthan_31_correct}
		\caption{Older than 31.5 years}
		\label{fig:olderthan31correct}
	\end{subfigure}
	\caption{Mean (green dot) and standard deviation (blue line) - correct elements per task, participants divided by mean age}
	\label{fig:splittedbymeanage}
\end{figure}



There is potentially a lot of work creating micro-tasks. The large task needs to be appropriately broken down to micro-tasks that are easy, enjoyable, and fast to solve. This breakdown requires design skills and proper tutorials and examples for new workers \citep{Schulze2012}. Humans make mistakes and these errors need to be detected. Quality mechanisms also needs to be determined, so that the output data is of high quality, without errors. To avoid putting too much emphasis on the preparations, guidelines can help to best utilize the preparation resources. The results from this thesis can be used as a guidance on how to break down the large task. It is not necessary to only use experienced individuals, especially if the 

The overall task result quality was good. One can doubt the difficulty of the questions given in the tasks. The design of the questions was maybe not realistic enough. 

The executed analyses in chapter four focus on the total time and correct elements in each task instead of separating in question one and two 