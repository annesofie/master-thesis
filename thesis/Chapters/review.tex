\chapter{Background}

Creating and maintaining real-world knowledge bases in a classical work environment demands a high cost, and is a cost that is often unnecessary [\citep{Meier2013}, p. 134]. Alternative approaches are to rely on the knowledge of open crowds, volunteer contributions, or services like micro-tasking platforms where there are people ready to work on the tasks given to them [\citep{Meier2013}, p. 134].   

Today, geospatial data is more available than ever. Governments are releasing more and more data and the OpenStreetMap database is still growing. While general data availability is increasing, the quality of the data is not necessarily perfect and manual pre-processing is often necessary before using it \citep{Difallah2015}.  Pre-processing of the data can require much time and high costs. By exploiting both machines and people through the appropriate platform, the cost can decrease and the quality increase. The author will argue in this chapter that combining machines and people is often a better and faster solution than a fully-automatic or fully-manual approach and implementing such an approach into a micro-tasking platform can be a good solution. 

\section{Why not just use machine learning?}
%hvorfor kan man ikke bare bruke ai? Skrive et avsnitt 3 ai bedrifter har microtasking i scopet ditt, nevne dette i introen
%https://pdfs.semanticscholar.org/d23b/1e9285d1d4b487e504d728d023d779d0ae65.pdf
%"Machine learning" was originally defined as "... artificial generation of knowledge from experienced". The first machine-learning procedures could play the game of checkers. The computer could be programmed so that it learned to play a better game of checkers than by the person who wrote the program after only 8-10 hours of training \citep{Samuel1959}. 

Machine learning give computers the ability to learn without being explicitly programmed. It involves computer intelligence, but the computer do not know the answers up front \citep{StanfordUniversity2017}. Machine-learning algorithms have enormous problems when the contextual information is lacking. Without a pre-set of rules, a machine has trouble solving the problem. Machines do not have creativity, which is required to solve complex problems \citep{Holzinger2016a}. According to the company "Mighty AI", humans cannot be removed from Artificial Intelligence training loops. They believe that humans will continue to play a crucial role in creating training data for the algorithms. %*Riktig å skrive algoritme her? kilde https://mty.ai/blog/why-you-cant-remove-humans-from-ai-training-loops/

It is suggested that machine learning accuracy should follow the Pareto 80:20 principle. Getting 80 \% accuracy can be fairly easy to accomplish, but the last 20 \% should to be handled by human input \citep{Biewald2015} \citep{Oppenheimer2017}. Human input can be to label the original training dataset or help correct inaccurate predictions outputted from the algorithm \citep{Oppenheimer2017}. A machine learning company called "developmentSEED" use a micro-tasking solution for cleaning their machine learning output data. They are using humans to get a more accurate output data faster. Skynet Scrubber, a GUI web application solution was developed to get the human input easier and faster (Their algorithm is called Skynet). In their blog, Derek Lieu writes: "Skynet gets more capable every day, but the output is still not perfect [..] We built Skynet Scrub so we could start using Skynet data sooner". 

\citep{Holzinger2016} claim that most people from the machine learning community are concentrating on \textit{automatic} machine learning by bringing the humans out of the process. When humans are out of the loop, the training data sets can be uncertain and incomplete, and the resulting algorithm can be questionable \citep{Holzinger2016}. By bringing humans back in the process, especially in domains where the data sets are questionable, for instance in the health domain, one enables what neither a human or a computer can do on their own \citep{Holzinger2016}. It is today possible to build hybrid human-machine systems that combine both the scalability of computers and the yet unmatched cognitive abilities of the human brain \citep{Difallah2016}. reCaptcha is a good example on how to exploit human cognitive abilities. Humans are through this solution identifying themselves as a human, and at the same time digitizing old books. In older books where the pages are bleached and yellow, machines struggle to understand what the words say. 750 000 000 individuals has helped digitize books through reCaptcha. %https://www.youtube.com/watch?v=cQl6jUjFjp4
"Computers are bad at finding patterns unless we have a well understood problem" quote Stephen Cohen, co-founder of Palantir Technologies, only humans can understand and frame a new problem. %https://www.youtube.com/watch?v=KvAXkA7u_A0
%Why Artificial Intelligence Needs a Task Theory + https://link.springer.com/chapter/10.1007/978-3-319-41649-6_12
Palantir Technologies believe in augmenting human intelligence, not replacing it. As \cite{Holzinger2013} say, "[...] the problem-solving knowledge is located in the human mind and - not in machines" and this is something we must acknowledge according to Holzinger. 

 
\section{Human computation}\label{sec:humancomputation}
%Problems computers cannot solve/struggle with (yet)
Human computing is, at its most general level, computation performed by humans and a human computation systems contains both humans and computers working together to solve difficult problems \citep{Schulze2012}. The author argue that utilizing the human processing power is still important. Humans are necessary even though our computers are becoming more and more complex. Traditional approaches to solving problems are to focus on improving the software, but often a solution that uses humans cleverly by exploiting the human brain's cognitive abilities can sometimes create much faster and better results than a software. One of the pioneers of crowdsourcing, Luis von Ahn, wanted to find a cheep and effective way to label images \citep{VonAhn2008}. The solution was to exploit the use of a game-like approach in a non-game context to motivate individuals to label the images though a game. This approach is called gamification \citep{Huotari2017}. The game was called "The ESP game" and solved the problem of labeling images with words. Most images don't have a proper caption associated with them and this makes it difficult to create search engines for images for instance. A fast and cheap method of labeling images is by using humans cleverly, humans can very easily see if the image contains a dog or cat for instance. Through "The ESP game" humans where labeling images without even knowing it, they only played a fun game. Within a few months, the game collected more than 40 million image labels \citep{VonAhn2008}, and they didn't even have to pay them doing it. Another game that was created by Luis Von Ahn is called "Peekaboom", this is also using a gamification approach. Here the players would locate objects in images. Such information is very useful in computer vision research for instance \citep{VonAhn2008}. Human computation is one of the major areas where the gamification approach has been employed \citep{Morschheuser2016}. Each human performs a small part of a massive computation task.  

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{../../../../../Desktop/humancomp_crowdsourcing}
	\caption{Collective intelligence \citep{Quinn2011}}
	\label{fig:humancompcrowdsourcing}
\end{figure}

Human computation, a term introduced by Luis von Ahn, refers to, according to \cite{Quinn2011}, a distributed system that combine the strengths of humans and computers to accomplish tasks that neither can do alone. To make human computation in crowdsourcing effective one need to know how the results can be optimally acquired from humans and how the results can be integrated into productive environments without having to change established workflows and practices [\citep{Meier2013}, p. 134]. Gamification can be one solution on how to make human computation in crowdsourcing effective \citep{Wang2017}. The author will argue in this chaper that micro-tasking can be another solution to effective crowdsourcing using humans cognitive abilities.

\section{Crowdsourcing}\label{sec:crowdsourcing}

%What is it
The first time the term "crowdsourcing" appeared was in Wired magazine article by Jeff Howe \citep{Howe2006}. Whereas human computing (section \ref{sec:humancomputation}) replaces computers with humans, crowdsourcing replaces traditional human workers with members of the public \citep{Quinn2011}. \cite{EYeka2015} state that 85 \% of the top global brands use crowdsourcing for various purposes. Crowdsourcing has become a widespread approach to dealing with machine-based computations where we leverage the human intelligence \citep{Gadiraju2015}. Crowdsourcing is an increasingly important concept, where the concept is the completion of large projects by combining small distributed contributions from the public \citep{Salk2016}. 

When the scope of a crowdsourced project is explicitly geographical, it is often called \textit{volunteered geographical information} (VGI). According to \cite{Salk2016}, the best known VGI project is OpenStreetMap (OSM). OSM is an open-source mapping project, where volunteers contribute with their local knowledge and mapping abilities. 

\section{Micro-tasking}\label{sec:microtasking}

%What is it
The simplest type of tasks are called micro-tasks and is illustrated in figure \ref{fig:microtaskingillustration}. Micro-tasks should not require any special training and a task should be completed within a couple of minutes \citep{Ipeirotis2010}. Problems that are suitable for solving through micro-tasking are those that are easy to distribute into a number of simple tasks, that can be completed in parallel in a relatively short period of time (from seconds to minutes), without requiring specific skills \citep{Sarasua2012}. Research has also demonstrated that micro-tasking is effective for far more complex problems when using sophisticated workflow management techniques. Micro-tasking can then be applied to a broader range of problems like: (1) completing surveys, (2) translating text between two languages, (3) matching pictures of people, (4) summarizing text \citep{Bernstein2015a}, etc. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{../../../../../Desktop/microtasking_illustration}
	\caption{Micro-tasking \citep{Michelucci2015}}
	\label{fig:microtaskingillustration}
\end{figure}

Micro-tasking and human computation are closely related. In the "Handbook of Human Computation", micro-tasking is present in the \textit{Human Computation for Disaster Response} chapter [\citep{Meier2013}, p. 95-105], as well as in several other chapters. In the \textit{Human Computation for Disaster Response} chapter they give an overview of how human computation methods, such as paid micro-tasks, could be used to help in major disasters. In 2012, Philippines was struck by a typhoon called Ruby, devastating large regions. With the help of CrowdFlower micro-tasking platform, the workers collected over 20 000 tweets related to the typhoon and identified the tweets containing links to either photos or video footage from the damaged areas. The relevant tweets were uploaded to the CrowdCrafting micro-tasking platform were volunteers both tagged and geo-tagged each photo and video if they portrayed evidence of damage. Within 12 hours a dataset of 100 georeferenced images and videos were collected. It resulted in a very detailed crisis map shown in figure \ref{fig:mm-ruby-tweet-crisis-map}. This was the first official crisis-map based solely on social media content [\citep{Meier2013}, p. 101]. In the aftermath of this crisis, an algorithm was developed to automatically detect tweets that link to photos and videos, which freed more time for the volunteers to georeference and tag more images and videos, since the algorithm detected them \citep{Meier2014}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{../../papers/mm-ruby-tweet-crisis-map}
	\caption[Crisis map \citep{Meier2014}]{Typhoon Ruby Crisis map \citep{Meier2014}}
	\label{fig:mm-ruby-tweet-crisis-map}
\end{figure}

Micro-task crowdsourcing refers to a problem-solving model in which a problem or task is outsourced to a distributed group of people by splitting the task or problem into smaller sub-tasks or sub-problems. The sub-tasks or sub-problems are then solved by multiple workers independently, often in return for a reward \citep{Sarasua2012}. Thanks to micro-tasking platforms as Amazon's Mechanical Turk (MTurk; www.mturk.com), it is possible to build a hybrid human-machine system that combines the scalability of computers with the yet unmatched cognitive abilities of the human brain \citep{Difallah2016}. \cite{Gadiraju2015} findings when analyzing data from MTurk, indicate rapid growth in micro-task crowdsourcing. With the establishment of micro-task crowdsourcing platforms as MTurk and CrowdFlower (www.crowdflower.com), micro-tasking is much more accessible. Micro-tasking practitioners are actively turning towards paid crowdsourcing to solve data-centric tasks that require human input \citep{Gadiraju2015}. Most cases of micro-tasking combine human computation abilities with crowdsourcing.

Many human computation systems use crowdsourcing platforms to recruit workers \citep{Schulze2012}. In machine-learning algorithms, combining human computation abilities and crowdsourcing with fast machine learning algorithms can be proven to be a success so far. An example is the team "Tomnod" \footnote{Tomnod is a team of volunteers who work together to identify important objects and interesting places in satellite images; www.tomnod.com} who did a project in Australia where they combined human computation, crowdsourcing, and machine learning to locate swimming pools \citep{Kostas2016}. The machine learning algorithm created polygons where there was likely to be a swimming pool inside. Then the users participating in finding the swimming pools were only shown polygons the algorithm had created, minimizing the search area, shown in figure \ref{fig:swimmingpoolpolygon3}. This approach was used in order to reduce the number of user votes required to classify the pools but yet obtain a sufficient confidence \citep{Kostas2016}. This is a good example of micro-tasking. Instead of serving the users with satellite photos of huge areas, the photos was divided into polygons created by an algorithm. The "Tomnod" tesm divided the task into smaller tasks, they micro-tasked the work. The resulting dataset was then used to train a swimming pool detecting convolutional neural network \citep{Nikki2016}.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{../../../../../Desktop/swimmingpoolpolygon3}
	\caption[Swimming pools  \citep{Nikki2016}]{Polygons created by an algorithm that maybe contain pools \citep{Nikki2016}}
	\label{fig:swimmingpoolpolygon3}
\end{figure}

Most cases of micro-tasking usage exploit the large volume capabilities machines have and the cognitive capabilities of humans \citep{Difallah2016}. Micro-tasking has also been used to process queries. In the \cite{Franklin2011} paper they extended a traditional query engine with a small number of operations that requires human input by generating and submitting requests.  They used a micro-tasking platform to get the crowd to answer queries that cannot otherwise be answered. There are especially two cases where human input is needed: a) when the data is unknown or incomplete, b) when there is a need for subjective comparisons. They used auto-generated user interfaces and new query operators that can obtain human input via the interfaces. The \cite{Franklin2011} paper demonstrated that human input can be leveraged to dramatically extend the range of SQL-based query processing. People are good at comparing items, such as how well an image represents a particular concept. Humans are also good at finding relevant information with the help of search engines etc \citep{Franklin2011}. By utilizing these qualities in humans, like they did in \citep{Franklin2011} paper by developing a micro-tasking based implementation of query operations, a huge cost and time sparing potential can be utilized. 

One of the advantages of micro-tasking platforms like "Tomnod" and "CrowdFlower", that is mentioned by \cite{Meier2013} (p. 99), is the built-in quality control mechanisms that ensure a relatively high quality of output data. They set a review constraint, for instance in a project where they tagged satellite imagery of Somalia each unique image was reviewed by at least three different volunteers and only when all three agreed on type and location it was approved. 

\subsection{Micro-tasking workforce}
It is said that crowdsourcing is radically changing the nature of work \cite{Deng2016a}. Traditional workers are restricted to offices and arranged office hours. With crowdsourcing, through for instance micro-tasking platforms, the workers can choose when to work, and even better: which jobs to perform. This appears very attractive, but is it only on the surface? 

According to \cite{Deng2016a}, evidence indicates that crowdsourcing is radically changing people's perspectives on how to manage their work-life balance. Compared to ''traditional" work tasks, the micro-tasks are simple and fast to finish (within a couple of minutes). The worker is also often compensated with tiny rewards every time they complete a micro-task. The workers are then rewarded often, which is motivating. 

Individuals who perform micro-tasks for micropayment is called \textit{crowd workers} by \citep{Deng2016a}. A study done on workers in the micro-tasking platform MTurk (section \ref{sec:mturk}), says that the workers are representative for the general Internet user population, but are generally younger and have lower incomes and smaller families \citep{Ipeirotis2010}.  

\subsection{Micro-tasking platforms}

%Note that like Tomnod, both CrowdFlower and CrowdCrafting also have built-in quality control mechanisms [\citep{Meier2013}, p. 101]. 

\subsubsection[MTurk]{Amazon's Mechanical Turk}\label{sec:mturk}
Amazon's Mechanical Turk (MTurk) is one of the biggest (if not the biggest) micro-tasking platform today. It provides the infrastructure, connectivity and payment mechanisms so that hundreds of thousands of people can perform micro-tasks on the Internet and get paid for it. MTurk is used for many different tasks that are easier for people than computers. It contains simple tasks such as labeling or segmenting images or tagging content, to more complex tasks such as translating or even editing text \citep{Franklin2011}.  In the marketplace, employers are known as requesters and they post tasks, called \textit{human intelligence tasks} (HIT's). The HIT's are then picked up by online users, \textit{crowd workers}, who complete the tasks in exchange for a small payment (a few cents per HIT) \citep{Ipeirotis2010}. 

%\subsubsection{Soylent}\label{sec:soylent}
%Is a word processing interface that enables writers to call on Mechanical Turk workers to shorten, proofread, and edit parts of their document on demand. To improve the quality of the work, the Soylen team introduced the Find-Fix-Verify crowd programming pattern. This architecture splits tasks into a series of generating and review stages \citep{Bernstein2015a}. 

\subsubsection{Tasking manager}\label{sec:taskingmanager}
The Tasking Manager tool is OpenStreetMap's micro-tasking platform. It was created in the aftermath of the Haiti earthquake in 2010 \citep{Palen2015}. The tool is used to coordinate satellite image tracing projects. The tool sorts the area covered by the satellite images into grids so that multiple people can map the same area at the same time. Each person works at one grid each, this way they don't map the same areas. This is an very effective approach to coordinate the crowd participating in the mapping. The tasking manager is mainly used by the \textit{Humanitarian OpenStreetMap Team} (HOT). This platform do not have a rewarding system og a gamification approach. It is solely based on volunteer contributors. The page shows which areas need to be mapped and which areas need the mapping validated by others. 

There are also other tools in OpenStreetMap. Tofix etcetc. 

\subsubsection{CrowdFlower}  
CrowdFlower is a company that wants to help businesses take advantage of crowdsourcing and/or human computation. They act as an intermediary for these businesses \citep{Quinn2011}. CrowdFlower receives tasks from businesses wanting to crowdsource their work or problems. A project done for eBay exploited CrowdFlower's large online contributor pool and completed the tasks given to them from eBay five times faster than a traditional outsourcing team. eBay got a solution that was optimized for both quality and cost. CrowdFlower works with a variety of services to get connected with workers (i.e MTurk) \citep{Quinn2011}. 

What's special with CorwdFlower is their close ties with AI technology and a crowdsourced workforce. Their costumers are allowed to perform tasks with algorithms and machine learning, but bring in human judgement when they're not confident on the technology and the human work can make the algorithms smarter \citep{Ha2016}.  Founder of CrowdFlower says that "self-driving cars have gotten pretty good at recognizing many of the objects they encounter on the street, [..] they can still struggle with tricky things like “a person in a Halloween costume dressed as a stationary object, or a pole with a person painted on it,” which is where CrowdFlower comes in." \citep{Ha2016}.

%\subsubsection{CrowdMap}
%CrowdMap is implemented using CrowdFlower. 
%Is an approach to integrate human and computational intelligence in ontology \footnote{\label{ontology} Ontology is a formal naming and definition of types, properties, and interrelationships of the entities that really or fundamentally exists for a particular domain of discourse. Ontologies are created to limit complexity and to organize information. The ontology can then be applied to problem-solving. } alignment tasks via microtask crowdsourcing \citep{Sarasua2012}. Ontology is still (2012) one of those problems that we cannot automate completely and by having a human in the loop might increase the quality of the results of machine-driven approaches. 

\section{Building imports using micro-tasking}
%Bygningsimportene til la og ny, her brukte de microtasking til deler av importen
In OpenStreetMap, at least to building imports was successfully completed using micro-tasking. The tasking manager platform (\ref{sec:taskingmanager}) was used to organize the import. 


\subsection{Challenges}
Getting enough people to use the micro-tasking platforms is crucial for its success. Most of the platforms mentioned in this chapter give payments to the workers. Another option is to make the platform as a game, which is also shown in this chapter. Creating a micro-tasking platform without payments or gamification factors the page is likely to have a short life, even though the tasking manager, supported by HOT, is an exception to this rule. 

A problem when combining machines and humans is that machines can do their operations in real-time, while humans are unpredictable, they can come and go as they wish. This creates a gap where the micro-tasking platforms cannot guarantee on the task completion time \citep{Difallah2016}. 

The human computation abilities can also be overestimated. During the classification of swimming pools in Australia, the Tomnod team faced some unexpected challenges. As described in section \ref{sec:microtasking}, they used the crowd to classify if a polygon contained a swimming pool or not, an algorithm had pointed out the polygons first. When reviewing a random sample from the result, they found an indication that 26\% of polygons that contained a pool were identified as not containing pools by the crowd \citep{Kostas2016}.  Further studies also showed that the guilty part was the crowd, the algorithm had correctly detected polygons containing pools. In a case where the algorithm was 85\% confident that the polygon contained a pool, only one voted 'yes', six voted 'no, this polygon do not contain a pool'. The solution was to combine the human verdict with the machine's prediction. This example shows that it is important to use the right combination of humans and machines. Tasks that at first seems simple to do for humans, may be more challenging that expected.  Basic object detection using machine learning perform very well when used together with human operations. 

It is important that operations added to a micro-tasking platform consider the talents and limitations of human workers \citep{Franklin2011} and this is what this thesis try to examine. What are the limitations of human workers when dealing with maps and geospatial data. It has been shown that crowds can be "programmed" to execute classical algorithms such as Quicksort, but such use of available resources is neither performant nor cost-efficient \citep{Franklin2011}. 


New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.” %http://blog.algorithmia.com/introduction-to-deep-learning-2016/

