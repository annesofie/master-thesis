\chapter{Background}

Creating and maintaining real-world knowledge bases in a classical work environment demands a high cost, and is a cost that is often unnecessary [\citep{Meier2013}, p. 134]. Alternative approaches are to rely on the knowledge of open crowds, volunteer contributions, or services like micro-tasking platforms where there are people ready to work on the tasks given to them.   

Today, geospatial data is more available than ever. Governments are releasing more and more data and the OpenStreetMap database is still growing. While general data availability is increasing, the quality of the data is not necessarily perfect and manual pre-processing is often necessary before using it \citep{Difallah2015}.  Pre-processing of the data can require much time and high costs. By exploiting both machines and people through the appropriate platform, the cost can decrease and the quality increase. As you will read in this chapter, combining machines and people is often a better and faster solution than a fully-automatic or fully-manual approach and implementing such an approach into a micro-tasking platform can be a good solution. 

\section{Human computation}\label{sec:humancomputation}
%Problems computers cannot solve/struggle with (yet)
Human computing is, at its most general level, computation performed by humans. It is tasks that computers cannot yet perform. Utilizing the human processing power is still important. Humans are necessary even though our computers are becoming more and more complex. Traditional approaches to solving problems are to focus on improving the software, but often a solution that uses humans cleverly by exploiting the human brain's cognitive abilities can sometimes create much faster and better results than a software. One of the pioneers of crowdsourcing, Luis von Ahn, created a game called "The ESP game". HVA ER GAMIFICATION It solves the problem of labeling images with words. Most images don't have a proper caption associated with them and this makes it difficult to create search engines for images for instance. A fast and cheap method of labeling images is by using humans cleverly, humans can very easily see if the image contains a dog or cat for instance. Through "The ESP game" humans where labeling images without even knowing it, they only played a fun game. Within a few months, the game collected more than 40 million image labels \citep{VonAhn2008}, and they didn't even have to pay them doing it. Another game that was created by Luis Von Ahn is called "Peekaboom". Here the players would locate objects in images. Such information is very useful in computer vision research for instance \citep{VonAhn2008}. Both games exploited humans abilities in a very clever way, our vision is still better than computers abilities to recognize items in images. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{../../../../../Desktop/humancomp_crowdsourcing}
	\caption{Collective intelligence \citep{Quinn2011}}
	\label{fig:humancompcrowdsourcing}
\end{figure}

Human computation, a term introduced by Luis von Ahn, refers to according to \cite{Quinn2011} a distributed system that combine the strengths of humans and computers to accomplish tasks that neither can do alone. To make human-computation in crowdsourcing effective one need to know how the results can be optimally acquired from humans and how the results can be integrated into productive environments without having to change established workflows and practices [\citep{Meier2013}, p. 134]. 

\section{Crowdsourcing}\label{sec:crowdsourcing}

%What is it
The first time the term "crowdsourcing" appeared was in Wires magazine article by Jeff Howe \citep{Howe2006}. Whereas human computing (section \ref{sec:humancomputation}) replaces computers with humans, crowdsourcing replaces traditional human workers with members of the public \citep{Quinn2011}. \cite{EYeka2015} state that 85 \% of the top global brands use crowdsourcing for various purposes. Crowdsourcing has become a widespread approach to dealing with machine-based computations where we leverage the human intelligence \citep{Gadiraju2015}. Crowdsourcing is an increasingly important concept, where the concept is the completion of large projects by combining small distributed contributions from the public \citep{Salk2016}. 

When the scope of a crowdsourced project is explicitly geographical, it is often called \textit{volunteered geographical information} (VGI). According to \cite{Salk2016}, the best known VGI project is OpenStreetMap (OSM). OSM is an open-source mapping project, where volunteers contribute with their local knowledge and mapping abilities. 

\section{Micro-tasking}\label{sec:microtasking}

%What is it
The simplest type of tasks are called micro-tasks and is illustrated in figure \ref{fig:microtaskingillustration}. Micro-tasks should not require any special training and a task should be completed within a couple of minutes \citep{Ipeirotis2010}. Problems that are suitable for solving through micro-tasking are those that are easy to distribute into a number of simple tasks, that can be completed in parallel in a relatively short period of time (from seconds to minutes), without requiring specific skills \citep{Sarasua2012}. Research has also demonstrated that micro-tasking is effective for far more complex problems when using sophisticated workflow management techniques. Micro-tasking can then be applied to a broader range of problems like: (1) completing surveys, (2) translating text between two languages, (3) matching pictures of people, (4) summarizing text \citep{Bernstein2015a}, etc. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{../../../../../Desktop/microtasking_illustration}
	\caption{Micro-tasking \citep{Michelucci2015}}
	\label{fig:microtaskingillustration}
\end{figure}

Micro-tasking and human computation are closely related. In the "Handbook of Human Computation", micro-tasking is present in the \textit{Human Computation for Disaster Response} chapter [\citep{Meier2013}, p. 95-105], as well as in several other chapters. In the \textit{Human Computation for Disaster Response} chapter they give an overview of how human computation methods, such as paid micro-tasks, could be used to help in major disasters. In 2012, Philippines was struck by a typhoon called Ruby, devastating large regions. With the help of CrowdFlower micro-tasking platform, the workers collected over 20 000 tweets related to the typhoon and identified the tweets containing links to either photos or video footage from the damaged areas. The relevant tweets were uploaded to the CrowdCrafting micro-tasking platform were volunteers both tagged and geo-tagged each photo and video if they portrayed evidence of damage. Within 12 hours a dataset of 100 georeferenced images and videos were collected. It resulted in a very detailed crisis map shown in figure \ref{fig:mm-ruby-tweet-crisis-map}. This was the first official crisis-map based solely on social media content [\citep{Meier2013}, p. 101]. In the aftermath of this crisis, an algorithm was developed to automatically detect tweets that link to photos and videos, but it doesn't mean that micro-tasking work is not important anymore. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{../../papers/mm-ruby-tweet-crisis-map}
	\caption[Crisis map \citep{Meier2014}]{Typhoon Ruby Crisis map \citep{Meier2014}}
	\label{fig:mm-ruby-tweet-crisis-map}
\end{figure}

Micro-task crowdsourcing refers to a problem-solving model in which a problem or task is outsourced to a distributed group of people by splitting the task or problem into smaller sub-tasks or sub-problems. The sub-tasks or sub-problems are then solved by multiple workers independently, often in return for a reward \citep{Sarasua2012}. Thanks to micro-tasking platforms as Amazon's Mechanical Turk (MTurk; www.mturk.com), it is possible to build a hybrid human-machine system that combines the scalability of computers with the yet unmatched cognitive abilities of the human brain \citep{Difallah2016}. \cite{Gadiraju2015} findings when analyzing data from MTurk, indicate rapid growth in micro-task crowdsourcing. With the establishment of micro-task crowdsourcing platforms as MTurk and CrowdFlower (www.crowdflower.com), micro-tasking is much more accessible. Micro-tasking practitioners are actively turning towards paid crowdsourcing to solve data-centric tasks that require human input \citep{Gadiraju2015}. Most cases of micro-tasking combine human computation abilities with crowdsourcing.

In machine-learning algorithms, combining human computation abilities and crowdsourcing with fast machine learning algorithms has been a success. The team "Tomnod" \footnote{Tomnod is a team of volunteers who work together to identify important objects and interesting places in satellite images; www.tomnod.com} did a project in Australia where they combined human resources, crowdsourcing, and machine learning to locate swimming pools. The machine learning algorithm found polygons where there was likely to be a swimming pool. Then the users participating in finding the swimming pools were only shown polygons the algorithm had selected, minimizing the search area, shown in figure \ref{fig:swimmingpoolpolygon3}. This approach was used in order to reduce the number of user votes required to classify the pools but yet obtain a sufficient confidence \citep{Kostas2016}. This is a good example of micro-tasking. Instead of serving the users with satellite photos of huge areas, the photos was divided into polygons created by an algorithm. They then divided the task into smaller tasks, they micro-tasked the work. The resulting dataset was then used to train a swimming pool detecting convolutional neural network.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{../../../../../Desktop/swimmingpoolpolygon3}
	\caption[Swimming pools  \citep{Nikki2016}]{Polygons created by an algorithm that maybe contain pools \citep{Nikki2016}}
	\label{fig:swimmingpoolpolygon3}
\end{figure}

Most cases of micro-tasking usage exploit the large volume capabilities machines have and the cognitive capabilities of humans \citep{Difallah2016}. Micro-tasking has also been used to process queries. In the \cite{Franklin2011} paper they extended a traditional query engine with a small number of operations that requires human input by generating and submitting requests.  They used a micro-tasking platform to get the crowd to answer queries that cannot otherwise be answered. There are especially two cases where human input is needed: a) when the data is unknown or incomplete, b) when there is a need for subjective comparisons. They used auto-generated user interfaces and new query operators that can obtain human input via the interfaces. The \cite{Franklin2011} paper demonstrated that human input can be leveraged to dramatically extend the range of SQL-based query processing. People are good at comparing items, such as how well an image represents a particular concept. Humans are also good at finding relevant information with the help of search engines etc \citep{Franklin2011}. By utilizing these qualities in humans, like they did in \citep{Franklin2011} paper by developing a micro-tasking based implementation of query operations, a huge cost and time sparing potential can be utilized. 

One of the advantages of micro-tasking platforms like "Tomnod" and "CrowdFlower", that is mentioned by \cite{Meier2013} (p. 99), is the built-in quality control mechanisms that ensure a relatively high quality of output data. They set a review constraint, for instance in a project where they tagged satellite imagery of Somalia each unique image was reviewed by at least three different volunteers and only when all three agreed on type and location it was approved. 

\subsection{Micro-tasking workforce}
It is said that crowdsourcing is radically changing the nature of work \cite{Deng2016a}. Traditional workers are restricted to offices and arranged office hours. With crowdsourcing, through for instance micro-tasking platforms, the workers can choose when to work, and even better: which jobs to perform. This appears very attractive, but is it only on the surface? 

According to \cite{Deng2016a}, evidence indicates that crowdsourcing is radically changing people's perspectives on how to manage their work-life balance. Compared to ''traditional" work tasks, the micro-tasks are simple and fast to finish (within a couple of minutes). The worker is also often compensated with tiny rewards every time they complete a micro-task. The workers are then rewarded often, which is motivating. 

Individuals who perform micro-tasks for micropayment is called \textit{crowd workers} by \citep{Deng2016a}. A study done on workers in the micro-tasking platform MTurk (section \ref{sec:mturk}), says that the workers are representative for the general Internet user population, but are generally younger and have lower incomes and smaller families \citep{Ipeirotis2010}.  

\subsection{Micro-tasking platforms}

%Note that like Tomnod, both CrowdFlower and CrowdCrafting also have built-in quality control mechanisms [\citep{Meier2013}, p. 101]. 

\subsubsection[MTurk]{Amazon's Mechanical Turk}\label{sec:mturk}
Amazon's Mechanical Turk (MTurk) is one of the biggest (if not the biggest) micro-tasking platform today. It provides the infrastructure, connectivity and payment mechanisms so that hundreds of thousands of people can perform micro-tasks on the Internet and get paid for it. MTurk is used for many different tasks that are easier for people than computers. It contains simple tasks such as labeling or segmenting images or tagging content, to more complex tasks such as translating or even editing text (\ref{sec:soylent}) \citep{Franklin2011}.  In the marketplace, employers are known as requesters and they post tasks, called \textit{human intelligence tasks} (HIT's). The HIT's are then picked up by online users, \textit{crowd workers}, who complete the tasks in exchange for a small payment (a few cents per HIT) \citep{Ipeirotis2010}. 

%\subsubsection{Soylent}\label{sec:soylent}
%Is a word processing interface that enables writers to call on Mechanical Turk workers to shorten, proofread, and edit parts of their document on demand. To improve the quality of the work, the Soylen team introduced the Find-Fix-Verify crowd programming pattern. This architecture splits tasks into a series of generating and review stages \citep{Bernstein2015a}. 

\subsubsection{Tasking manager}
The Tasking Manager tool is OpenStreetMap's micro-tasking platform. It was created in the aftermath of the Haiti earthquake in 2010 \citep{Palen2015}. The tool is used to coordinate satellite image tracing projects. The tool sorts the area covered by the satellite images into grids so that multiple people can map the same area at the same time. Each person works at one grid each, this way they don't map the same areas. This is an very effective approach to coordinate the crowd participating in the mapping. The tasking manager is mainly used by the \textit{Humanitarian OpenStreetMap Team} (HOT). This platform do not have a rewarding system og a gamification approach. It is solely based on volunteer contributors. The page shows which areas need to be mapped and which areas need the mapping validated by others. 

There are also other tools in OpenStreetMap. Tofix etcetc. 

\subsubsection{CrowdFlower}  
CrowdFlower is a company that wants to help businesses take advantage of crowdsourcing and/or human computation. They act as an intermediary for these businesses \citep{Quinn2011}. CrowdFlower receives tasks from businesses wanting to crowdsource their work or problems. A project done for eBay exploited CrowdFlower's large online contributor pool and completed the tasks given to them from eBay five times faster than a traditional outsourcing team. eBay got a solution that was optimized for both quality and cost. CrowdFlower works with a variety of services to get connected with workers (i.e MTurk) \citep{Quinn2011}. 

What's special with CorwdFlower is their close ties with AI technology and a crowdsourced workforce. Their costumers are allowed to perform tasks with algorithms and machine learning, but bring in human judgement when they're not confident on the technology and the human work can make the algorithms smarter \citep{Ha2016}.  Founder of CrowdFlower says that "self-driving cars have gotten pretty good at recognizing many of the objects they encounter on the street, [..] they can still struggle with tricky things like “a person in a Halloween costume dressed as a stationary object, or a pole with a person painted on it,” which is where CrowdFlower comes in." \citep{Ha2016}. 

%\subsubsection{CrowdMap}
%CrowdMap is implemented using CrowdFlower. 
%Is an approach to integrate human and computational intelligence in ontology \footnote{\label{ontology} Ontology is a formal naming and definition of types, properties, and interrelationships of the entities that really or fundamentally exists for a particular domain of discourse. Ontologies are created to limit complexity and to organize information. The ontology can then be applied to problem-solving. } alignment tasks via microtask crowdsourcing \citep{Sarasua2012}. Ontology is still (2012) one of those problems that we cannot automate completely and by having a human in the loop might increase the quality of the results of machine-driven approaches. 

\subsection{Challenges}
Getting enough people to use the micro-tasking platforms is crucial for its success. Most of the platforms mentioned in this chapter give payments to the workers. Another option is to make the platform as a game, which is also shown in this chapter. Creating a micro-tasking platform without payments or gamification factors the page is likely to have a short life, even though the tasking manager, supported by HOT, is an exception to this rule. 

A problem when combining machines and humans is that machines can do their operations in real-time, while humans are unpredictable, they can come and go as they wish. This creates a gap where the micro-tasking platforms cannot guarantee on the task completion time \citep{Difallah2016}. 

The human computation abilities can also be overestimated. During the classification of swimming pools in Australia, the Tomnod team faced some unexpected challenges. As described in section \ref{sec:microtasking}, they used the crowd to classify if a polygon contained a swimming pool or not, an algorithm had pointed out the polygons first. When reviewing a random sample from the result, they found an indication that 26\% of polygons that contained a pool were identified as not containing pools by the crowd \citep{Kostas2016}.  Further studies also showed that the guilty part was the crowd, the algorithm had correctly detected polygons containing pools. In a case where the algorithm was 85\% confident that the polygon contained a pool, only one voted 'yes', six voted 'no, this polygon do not contain a pool'. The solution was to combine the human verdict with the machine's prediction. This example shows that it is important to use the right combination of humans and machines. Tasks that at first seems simple to do for humans, may be more challenging that expected.  Basic object detection using machine learning perform very well when used together with human operations. 

It is important that operations added to a micro-tasking platform consider the talents and limitations of human workers \citep{Franklin2011} and this is what this thesis try to examine. What are the limitations of human workers when dealing with maps and geospatial data. It has been shown that crowds can be "programmed" to execute classical algorithms such as Quicksort, but such use of available resources is neither performant nor cost-efficient \citep{Franklin2011}. 

