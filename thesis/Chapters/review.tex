\chapter{Micro-tasking review}

Creating and maintaining real-world knowledge bases in a classical work environment demands a high cost, and is a cost that is often unnecessary [\citep{Meier2013}, p. 134]. Alternative approaches is to rely on the knowledge of open crowds, volunteer contributions, or services like micro-tasking platforms where there are people ready to work on the tasks given to them.   

Today, geospatial data is more available than ever. Governments are releasing more and more data and the OpenStreetMap database is still growing. While general data availability is increasing, the quality of the data is not necessarily perfect and manual pre-processing is often necessary before using it \citep{Difallah2015}.  Pre-processing of the data can require much time and high costs. By exploiting both machines and people through the appropriate platform, the cost can decrease and the quality increase. As you will read in this chapter, combining machines and people is often a better and faster solution than a fully-automatic or fully-manual approach and implementing such an approach into a micro-tasking platform can be a good solution. 

\section{Human computation}\label{sec:humancomputation}
%Problems computers cannot solve / struggle with (yet)
Human computing is, at its most general level, computation performed by humans. It is tasks that computers cannot yet perform. Utilizing the human processing power is still important. Humans are necessary even though our computers are becoming more and more complex. Traditional approaches to solving problems are to focus on improving the software, but often a solution that uses humans cleverly by exploiting the human brain's cognitive abilities can sometimes create much faster and better results than a software. One of the pioneers of crowdsourcing, Luis von Ahn, created a game called "The ESP game". It solves the problem of labeling images with words. Most images don't have a proper caption associated with them and this makes it difficult to search for images. A fast and cheap method of labeling images is by using humans cleverly, humans can very easily see if the image contains a dog or cat for instance. Through "The ESP game" humans where labeling images without even knowing it, they only played a fun game. Within a few months, the game collected more than 40 million image labels \citep{VonAhn2008}, and they didn't even have to pay them doing it. Another game that was created by Luis Von Ahn is called "Peekaboom". Here the players would locate objects in images. Such information is very useful in computer vision research for instance \citep{VonAhn2008}. Both games exploited humans abilities in a very clever way, our vision is still better than computers abilities to recognize items in images. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{../../../../../Desktop/humancomp_crowdsourcing}
	\caption{Collective intelligence \citep{Quinn2011}}
	\label{fig:humancompcrowdsourcing}
\end{figure}

Human computation, a term introduced by Luis von Ahn, refers to according to \cite{Quinn2011} a distributed system that combine the strengths of humans and computers to accomplish tasks that neither can do alone. To make human-computation in crowdsourcing effective one need to know how the results can be optimally acquired from humans and how the results can be integrated into productive environments without having to change established workflows and practices [\citep{Meier2013}, p. 134]. 

\section{Crowdsourcing}\label{sec:crowdsourcing}

%What is it
The first time the term "crowdsourcing" appeared was in Wires magazine article by Jeff Howe \citep{Howe2006}. Whereas human computing (section \ref{sec:humancomputation}) replaces computers with humans, crowdsourcing replaces traditional human workers with members of the public \citep{Quinn2011}. \cite{EYeka2015} state that 85 \% of the top global brands use crowdsourcing for various purposes. Crowdsourcing has become a widespread approach to dealing with machine-based computations where we leverage the human intelligence \citep{Gadiraju2015}. Crowdsourcing is an increasingly important concept, where the concept is the completion of large projects by combining small distributed contributions from the public \citep{Salk2016}. 

When the scope of crowdsourced project is explicity geographical, it is often called \textit{volunteered geographical information} (VGI). According to \cite{Salk2016}, the best known VGI proiect is OpenStreetMap (OSM). OSM is an open-source mapping project, where volunteers contribute with their local knowledge and mapping abilities. 

\section{Micro-tasking}\label{sec:microtasking}

%What is it
The simplest type of tasks are called micro-tasks and is illustrated in figure \ref{fig:microtaskingillustration}. Micro-tasks should not require any special training and a task should be completed within a couple of minutes \citep{Ipeirotis2010}. Problems that are suitable for solving through micro-tasking are those that are easy to distribute into a number of simple tasks, that can be completed in parallel in a relatively short period of time (from seconds to minutes), without requiring specific skills \citep{Sarasua2012}. Research has also demonstrated that micro-tasking is effective for far more complex problems when using sophisticated workflow management techniques. Micro-tasking can then be applied to a broader range of problems like: (1) completing surveys, (2) translating text between two languages, (3) matching pictures of people, (4) summarizing text \citep{Bernstein2015a}, etc. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{../../../../../Desktop/microtasking_illustration}
	\caption{Micro-tasking \citep{Michelucci2015}}
	\label{fig:microtaskingillustration}
\end{figure}

Micro-tasking and human computation are closely related. In the "Handbook of Human Computation", micro-tasking is present in the \textit{Human Computation for Disaster Response} chapter [\citep{Meier2013}, p. 95-105], as well as in several other chapters. In the \textit{Human Computation for Disaster Response} chapter they give an overview of how human computation methods, such as paid micro-tasks, could be used to help in major disasters. In 2012, Philippines was struck by a typhoon, devastating large regions. With the help of CrowdFlower micro-tasking platform, the workers collected over 20 000 tweets related to the typhoon and identified the tweets containing links to either photos or video footage from the damaged areas. The relevant tweets were uploaded to the CrowdCrafting micro-tasking platform where volunteers tagged each photo and video if they portrayed evidence of damage. 
  
Micro-tasking and human computation are closely related. In the "Handbook of Human Computation", micro-tasking is present in the \textit{Human Computation for Disaster Response} chapter [\citep{Meier2013}, p. 95-105], as well as in several other chapters. In the \textit{Human Computation for Disaster Response} chapter they give an overview of how human computation methods, such as paid micro-tasks, could be used to help in major disasters. In 2012, Philippines was struck by a typhoon called Ruby, devastating large regions. With the help of CrowdFlower micro-tasking platform the workers collected over 20 000 tweets related to the typhoon and identified the tweets containing links to either photos or video footage from the damaged areas. The relevant tweets were uploaded to the CrowdCrafting micro-tasking platform were volunteers both tagged and geo-tagged each photo and video if they portrayed evidence of damage. Within 12 hours a dataset of 100 georeferenced images and videos were collected. It resulted in a very detailed crisis map shown in figure \ref{fig:mm-ruby-tweet-crisis-map}. This was the first official crisis-map based solely on social media content [\citep{Meier2013}, p. 101]. In the aftermath of this crisis a algorithm to automatically detect tweets that link to photos and videos was developed, but it doesn't mean that micro-tasking work is not important any more. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{../../papers/mm-ruby-tweet-crisis-map}
	\caption{Typhoon Ruby Crisis map \citep{Meier2014}}
	\label{fig:mm-ruby-tweet-crisis-map}
\end{figure}

In machine-learning algorithms, combining human computation abilities with fast machine learning algorithms has been a success. The team "Tomnod" \footnote{Tomnod is a team of volunteers who work together to identify important objects and interesting places in satellite images; www.tomnod.com} did a project in Australia where they combined human resources and machine learning to locate swimming pools. The machine learning algorithm found polygons where there was likely to be a swimming pool. Then the users participating in finding the swimming pools where only shown polygons the algorithm had selected, minimizing the search area, shown in figure \ref{fig:swimmingpoolpolygon3}. This approach was used in order to reduce the number of user votes required to classify the pools but yet obtain a sufficient confidence \citep{Kostas2016}. This is a good example of micro-tasking. Instead of serving the users with satellite photos of huge areas, the photos was divided into polygons created by an algorithm. They then divided the task into smaller tasks, they micro-tasked the work. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{../../../../../Desktop/swimmingpoolpolygon3}
	\caption{Polygons with swimming pools created by the machine learning algorithm \citep{Nikki2016}}
	\label{fig:swimmingpoolpolygon3}
\end{figure}

Most cases of micro-tasking usage exploit the large volume capabilities machines have and the cognitive capabilities of humans \citep{Difallah2016}. Micro-tasking has been used to process queries. In the \cite{Franklin2011} paper they extended a traditional query engine with a small number of operations that requires human input by generating and submitting requests to a micro-tasking platform.  They use a micro-tasking platform to get the crowd to answer queries that cannot otherwise be answered. There are especially two cases where human input is needed: a) when the data is unknown or incomplete, b) when there is need for subjective comparisons. They used auto-generated user interfaces and new query operators that can obtain human input via the interfaces. The \cite{Franklin2011} paper demonstrated that human input can be leveraged to dramatically extend the range of SQL-based query processing. 

Micro-task crowdsourcing refers to a problem-solving model in which a problem or task is outsourced to a distributed group of people by splitting the task or problem into smaller sub-tasks or sub-problems. The sub-tasks or sub-problems are then solved by multiple workers independently, often in return for a reward \citep{Sarasua2012}. \cite{Gadiraju2015} findings when analyzing data from MTurk, indicate rapid growth in micro-task crowdsourcing. With the establishment of micro-task crowdsourcing platforms as Amazon's Mechanical Turk (MTurk; www.mturk.com) and CrowdFlower (www.crowdflower.com), micro-tasking is much more accessible. Micro-tasking practitioners are actively turning towards paid crowdsourcing to solve data-centric tasks that require human input \citep{Gadiraju2015}. 

Thanks to micro-tasking platforms as Amazon's Mechanical Turk (MTurk), it is possible to build a hybrid human-machine system that combines the scalability of computers with the yet unmatched cognitive abilities of the human brain \citep{Difallah2016}.

One of the advantages of micro-tasking platforms like "Tomnod" and "CrowdFlower", that is mentioned by \cite{Meier2013} (p. 99), is the built-in quality control mechanisms that ensure a relatively high quality of output data. They set a review constraint, for instance in a project where they tagget satellite imagery of Somalia each unique image was reviewed by at least three different volunteers and only when all three agreed on type and location it was approved. 

The future in micro-tasking "belongs to hybrid methodologies that combine human computation with advanced computing" \citep{Meier2013}. 

\subsection{Micro-tasking workers}
It is said that crowdsourcing is radically changing the nature of work \citep{Deng2016a}. Traditional workers are restricted to offices, and stipulated office hours. With crowdsourcing, through for instance micro-tasking platforms, the workers can choose when to work, and even better: which jobs to perform. This appears very attractive, but is it only on the surface? 

According to \cite{Deng2016a} evidence indicates that crowdsourcing is radically changing people's perspectives on how to manage their work-life balance. Compared to ''traditional" work tasks, the micro-tasks are simple and fast to finish (within a couple of minutes). The worker are also often compensated with tiny rewards every time they complete a micro-task. 
Rewarding the workers often, which is motivating. 

Individuals who perform micro-tasks for micropayment is called \textit{crowd workers} by \citep{Deng2016a}. A study done on workers in the micro-tasking platform MTurk (section \ref{sec:mturk}), says that the workers are representative for the general Internet user population, but are generally younger and have lower incomes and smaller families \citep{Ipeirotis2010}.  

\subsection{Micro-tasking platforms}
\subsubsection[MTurk]{Amazon's Mechanical Turk}\label{sec:mturk}
Amazon's Mechanical Turk (MTurk) is one of the biggest (if not the biggest) micro-tasking platform today. It provides the infrastructure, connectivity and payment mechanisms so that hundreds of thousands of people can perform micro-tasks on the Internet and get paid for it. MTurk is used for many different tasks that are easier for people than computers. In contains simple tasks such as labeling or segmenting images or tagging content, to more complex tasks such as translating or even edition text (\ref{sec:soylent}) \citep{Franklin2011}.  In the marketplace, employers are known as requesters and they post tasks, calles \textit{human intelligence tasks} (HIT's). The HIT's are then picked up by online users, \textit{crowd workers}, who complete the tasks in exchange for a small payment (a few cents per HIT) \citep{Ipeirotis2010}. 

\subsubsection{Soylent}\label{sec:soylent}
Is a word processing interface that enables writers to call on Mechanical Turk workers to shorten, proofread, and edit parts of their document on demand. To improve the quality of the work, the Soylen team introduced the Find-Fix-Verify crowd programming pattern. This architecture splits tasks into a series of generating and review stages \citep{Bernstein2015a}. 

\subsubsection{Tasking manager}
\subsubsection{Crowdflower}  
\subsubsection{CrowdMap}
CrowdMap is implemented using CrowdFlower. 
Is an approach to integrate human and computational intelligence in ontology \footnote{\label{ontology} Ontology is a formal naming and definition of types, properties, and interrelationships of the entities that really or fundamentally exists for a particular domain of discourse. Ontologies are created to limit complexity and to organize information. The ontology can then be applied to problem solving. } alignment tasks via microtask crowdsourcing \citep{Sarasua2012}. Ontology is still (2012) one of those problems that we cannot automate completely and having a human in the loop might increase the quality of the results of machine-driven approaches. 

\subsection{Usage}
A machine learning company called "developmentSEED" use a micro-tasking solution for cleaning machine learning output data. They have created a GUI web application solution called Skynet Scrubber. In their blog, Derek Lieu writes: "Skynet gets more capable every day, but the output is still not perfect [..] We built Skynet Scrub so we could start using Skynet data sooner". 

People are good at comparing items, such as how well an image represents a particular concept. We are also good at finding relevant information with the help of search engines etc \citep{Franklin2011}. By utilizing these qualities in humans, like they did in \citep{Franklin2011} paper by developing a micro-tasking based implementations of query operations, a huge cost and time sparing potential can be utilized. 

\subsection{Challenges}
Getting people to use the micro-tasking platforms. Most of the platforms mentioned in this chapter give payments to the workers. Another options is to make the platform as a game, which is also shown in this chapter. Creating a micro-tasking platform without payments og gamification factors the page is likely to have a short life, even though the tasking manager, supported by HOT, is an exception to this rule. 

When aiming towards wider adoption of crowdsourcing one have to be aware of the challenges of using it. It is important to remember that all tasks do not fit into the micro-tasking crowdworker model. Very complex tasks that can't be partitioned are not suitable for solving through micro-tasks. 

It is important that operations added to a micro-tasking platform considers the talents and limitations of human workers \citep{Franklin2011} and this is what this thesis try to examine. What is the limitations of human workers when dealing with geospatial data. It has been shown that crowds can be "programmed" to execute classical algorithms such as Quicksort, but such use of available resources is neither performant nor cost-efficient \citep{Franklin2011}. 

One problem is that machines can do their operations in real-time, while humans are unpredictable, they can come and go as they wish. This creates a gap where the micro-tasking platforms cannot guarantee on the task completion time \citep{Difallah2016}. 

During the classification of swimming pools in Australia the Tomnod team faced some unexpected challenges. As described in section \ref{sec:microtasking}, they used the crowd to classify if a polygon contained a swimming pool or not, an algorithm had pointed out the polygons first. When reviewing a random sample from the result, they found an indication that 26\% of polygons that contained a pool were identified as not containing pools \citep{Kostas2016}.  Further studies also showed that the guilty part was the crowd, the algorithm had correctly detected polygons containing pools. In a case where the algorithm was 85\% confident that the polygon contained a pool, only one voted 'yes', six voted 'no'. The solution was to combine the human verdict with the machine's prediction. This example show that it is important to use the right combination of humans and machines. Tasks that at first seems simple to do for humans, may be more challenging that expected.  Basic object detection using machine learning perform very well when used together with human operations. 

Advanced computing techniques such as Artificial Intelligence and Machine Learning is needed to build approaches that combine the power of people with the speed and scalability of automated algorithms \citep{Meier2013}. 
