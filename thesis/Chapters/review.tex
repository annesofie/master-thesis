\chapter{Micro-tasking review}

Today, geospatial data is more available than ever. Governments are releasing more and more data and OpenStreetMap is still growing. While general data availability is increasing, the quality of the data is not necessarily perfect and manual pre-processing is often necessary before using it \citep{Difallah2015}.  

\section{Human computation}\label{sec:humancomputation}
%Problems computers cannot solve / struggle with (yet)
Utilizing the human processing power is still important. Humans are necessary even though our computers are becoming more and more complex. Traditional approaches for solving problems is to focus on improving the software, but often a solution that uses humans cleverly by exploiting the human brains cognitive abilities can create much faster and better results than a software. One of the pioneers of crowdsourcing, Luis von Ahn, created a game called "The ESP game". It solves the problem of labelling images with words. Most images don't have a proper caption associated with them. A fast and cheap method of labelling images is by using humans cleverly. Through "The ESP game" the players label images without even knowing it, they only played a fun game. Within a few months the game collected more than 40 million image labels \citep{VonAhn2008}. Another game that was created by Luis Von Ahn is called "Peekaboom". Here the players would locate objects in images. Such information is very useful in computer vision research for instance \citep{VonAhn2008}.   

Human computation, a term introduced by Luis von Ahn, refers to according to \cite{Quinn2011} a distributed system that combine the strengths of human and computers to accomplish tasks that neither can do alone. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{../../../../../Desktop/humancomp_crowdsourcing}
	\caption{Collective intelligence \citep{Quinn2011}}
	\label{fig:humancompcrowdsourcing}
\end{figure}


\section{Crowdsourcing}\label{sec:crowdsourcing}

%What is it
The first time the term "crowdsourcing" appeared was in Wires magazine article by Jeff Howe \citep{Howe2006}. Whereas human computing (section \ref{sec:humancomputation}) replaces computers with humans, crowdsourcing replaces traditional human workers with members of the public \citep{Quinn2011}. \cite{EYeka2015} state that 85 \% of the top global brands use crowdsourcing for various purposes. Crowdsourcing has become an widespread approach to dealing with machine-based computations where we leverage the human intelligence \citep{Gadiraju2015}.

\cite{Gadiraju2015} categorize typically crowdsourced tasks into six top level classes. Interesting classes within geospatial data is \textit{Verification and validation}, \textit{Interpretation and analysis} and \textit{Content creation}. There are examples of all three task classes in geospatial crowdsourcing. During imports of large datasets into OpenStreetMap crowdsourcing is used to validate the new data. In humanitarian OpenSteetMap they use micro-tasking to create geospatial data in areas during crisis to support the help organizations. In machine learning process teams are starting to use micro-tasks too both validate the created data and also create test datasets to the algorithms.\textit{ Interpretation and analysis} tasks rely on the individual to use their interpretation skills during task completion. The task can be to choose between two layers, and decide which is best. This is the task-class used in this thesis during the survey. More in section \ref{sec:survey}.

\section{Micro-tasking}

%What is it
Micro-tasks should not require any special training and a task should be completed within a couple of minutes \citep{Ipeirotis2010}.  

With the establishment of micro-task crowdsourcing platforms as Amazon's Mechanical Turk (MTurk; www.mturk.com) and CrowdFlower (www.crowdflower.com), micro-tasking is much more accessible. Micro-tasking practitioners are actively turning towards paid crowdsourcing to solve data-centric tasks that require human input \citep{Gadiraju2015}. 

Micro-tasking has already been used to process queries for instance. In the \cite{Franklin2011} paper they extended a traditional query engine with a small number of operations that requires human input by generating and submitting requests to a micro-tasking platform. Most cases of micro-tasking usage exploit the large volume capabilities machines have and the cognitive capabilities of humans \citep{Difallah2016}.  

The task refers to the activity, production or service the company or organization wants to have done. \cite{Gadiraju2015} findings when analyzing data from MTurk, indicate rapid growth in micro-task crowdsourcing. 

Micro-task crowdsourcing refers to a problem-solving model in which a problem or task is outsourced to a distributed group of people by splitting the task or problem into smaller sub-tasks or sub-problems. The sub-tasks or sub-problems are then solved by multiple workers independently, often in return for a reward \citep{Sarasua2012}. 

Problems that are suitable for solving through micro-tasking are those that are easy to distribute into a number of simple tasks, that can be completed in parallel in a relatively short period of time (from seconds to minutes), without requiring specific skills \citep{Sarasua2012}. Research has also demonstrated that micro-tasking is effective for far more complex problems when using sophisticated workflow management techniques. MIcro-tasking can then be applied to a broader range of problems like: (1) completing surveys, (2) translating text between two languages, (3) matching pictures of people, (4) summarizing text \citep{Bernstein2015a}, etc. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{../../../../../Desktop/microtasking_illustration}
	\caption{Micro-tasking \citep{Michelucci2015}}
	\label{fig:microtaskingillustration}
\end{figure}


\subsection{Human Intelligence tasks}
Thanks to micro-tasking platforms as Amazon's Mechanical Turk (MTurk), it is possible to build hybrid human-machine system that combines the scalability of computers with the yet unmatched cognitive abilities of the human brain \citep{Difallah2016}.

\subsection{Micro-tasking platforms}
\subsubsection[MTurk]{Amazon's Mechanical Turk}
Amazon's Mechanical Turk (MTurk) is one of the biggest (if not the biggest) micro-tasking platform today. It provides the infrastructure, connectivity and payment mechanisms so that hundreds of thousands of people can perform micro-tasks on the Internet and get paid for it. MTurk is used for many different tasks that are easier for people than computers. In contains simple tasks such as labeling or segmenting images or tagging content, to more complex tasks such as translating or even edition text (\ref{sec:soylent}) \citep{Franklin2011}.  

\subsubsection{Soylent}\label{sec:soylent}
Is a word processing interface that enables writers to call on Mechanical Turk workers to shorten, proofread, and edit parts of their document on demand. To improve the quality of the work, the Soylen team introduced the Find-Fix-Verify crowd programming pattern. This architecture splits tasks into a series of generating and review stages \citep{Bernstein2015a}. 

\subsubsection{Tasking manager}
\subsubsection{Crowdflower}
\subsubsection{CrowdMap}

CrowdMap is implemented using CrowdFlower. 
Is an approach to integrate human and computational intelligence in ontology \footnote{\label{ontology} Ontology is a formal naming and definition of types, properties, and interrelationships of the entities that really or fundamentally exists for a particular domain of discourse. Ontologies are created to limit complexity and to organize information. The ontology can then be applied to problem solving. } alignment tasks via microtask crowdsourcing \citep{Sarasua2012}. Ontology is still (2012) one of those problems that we cannot automate completely and having a human in the loop might increase the quality of the results of machine-driven approaches. 





\subsection{Usage}
A machine learning company called "developmentSEED" use a micro-tasking solution for cleaning machine learning output data. They have created a GUI web application solution called Skynet Scrubber. In their blog, Derek Lieu writes: "Skynet gets more capable every day, but the output is still not perfect [..] We built Skynet Scrub so we could start using Skynet data sooner". 

People are good at comparing items, such as how well an image represents a particular concept. We are also good at finding relevant information with the help of search engines etc \citep{Franklin2011}. By utilizing these qualities in humans, like they did in \citep{Franklin2011} paper by developing a micro-tasking based implementations of query operations, a huge cost and time sparing potential can be utilized. 

\subsection{Challenges}
When aiming towards wider adoption of crowdsourcing one have to be aware of the challenges of using it. It is important to remember that all tasks do not fit into the micro-tasking crowdworker model. Very complex tasks that can't be partitioned are not suitable for solving through micro-tasks. 

It is important that operations added to a micro-tasking platform considers the talents and limitations of human workers \citep{Franklin2011} and this is what this thesis try to examine. What is the limitations of human workers when dealing with geospatial data. It has been shown that crowds can be "programmed" to execute classical algorithms such as Quicksort, but such use of available resources is neither performant nor cost-efficient \citep{Franklin2011}. 

One problem is that machines can do their operations in real-time, while humans are unpredictable, they can come and go as they wish. This creates a gap where the micro-tasking platforms cannot guarantee on the task completion time \citep{Difallah2016}. 
