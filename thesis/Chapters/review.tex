\chapter{Micro-tasking review}
Today, geospatial data is more available than ever. Governments are releasing more and more data and OpenStreetMap is still growing. While general data availability is increasing, the quality of the data is not necessarily perfect and manual pre-processing is often necessary before using it \citep{Difallah2015}.  Pre-processing of the data requires both machines and people. As you will read in this chapter combining machines and people is often a better and faster solution than a fully-automatic or fully-manual approach. 

A task can traditionally be divided by time, place, person, object and skill [\citep{Meier2013}, p. 13]. A task can be created by identifying the time it will require, the place where it must be done, the people who need to do the task, the object on which the work is done and finally, the skill needed for the task. Today we have technology that can create and move tasks around based on the four first categories.  Technology can allocate tasks based on the deadline and time the task requires, it can also establish communication between any team of people dependent on which people the task requires. One thing that technology can't do is change the skill of individual workers, though it can only connect people with different skills to work on the same tasks [\citep{Meier2013}, p. 14]. Crowdsourcing moved beyond this and looks at the skills of individual workers and the problem that needs to be solved and combines the best skills of workers. The division of labor by skill has more economic impact than the other four categories [\citep{Meier2013}, p. 15]. Crowdsourcing is a way of refactoring work in a way that exploits the workers flexibility and gets the right skills to the right part of the problem. To get the right worker to the right part of the problem it needs to be partitioned into smaller parts, to make it easier to distribute the problem. This can be done through micro-tasking, also called "smart crowdsourcing" by Patrick Meier \citep{Meier2013a}. This chapter will give a thorough introduction to micro-tasking and hopefully make it more clearer what this thesis aim is. 

\section{Human computation}\label{sec:humancomputation}
%Problems computers cannot solve / struggle with (yet)
Utilizing the human processing power is still important. Humans are necessary even though our computers are becoming more and more complex. Traditional approaches to solving problems are to focus on improving the software, but often a solution that uses humans cleverly by exploiting the human brain's cognitive abilities can sometimes create much faster and better results than a software. One of the pioneers of crowdsourcing, Luis von Ahn, created a game called "The ESP game". It solves the problem of labeling images with words. Most images don't have a proper caption associated with them. A fast and cheap method of labeling images is by using humans cleverly. Through "The ESP game" the humans are labeling images without even knowing it, they only played a fun game. Within a few months, the game collected more than 40 million image labels \citep{VonAhn2008}, and they didn't even have to pay them for doing it. Another game that was created by Luis Von Ahn is called "Peekaboom". Here the players would locate objects in images. Such information is very useful in computer vision research for instance \citep{VonAhn2008}.   

Human computation, a term introduced by Luis von Ahn, refers to according to \cite{Quinn2011} a distributed system that combine the strengths of humans and computers to accomplish tasks that neither can do alone. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{../../../../../Desktop/humancomp_crowdsourcing}
	\caption{Collective intelligence \citep{Quinn2011}}
	\label{fig:humancompcrowdsourcing}
\end{figure}


\section{Crowdsourcing}\label{sec:crowdsourcing}

%What is it
The first time the term "crowdsourcing" appeared was in Wires magazine article by Jeff Howe \citep{Howe2006}. Whereas human computing (section \ref{sec:humancomputation}) replaces computers with humans, crowdsourcing replaces traditional human workers with members of the public \citep{Quinn2011}. \cite{EYeka2015} state that 85 \% of the top global brands use crowdsourcing for various purposes. Crowdsourcing has become a widespread approach to dealing with machine-based computations where we leverage the human intelligence \citep{Gadiraju2015}.

\cite{Gadiraju2015} categorize typically crowdsourced tasks into six top-level classes. Interesting classes within geospatial data is \textit{Verification and validation}, \textit{Interpretation and analysis} and \textit{Content creation}. There are examples of all three task classes in geospatial crowdsourcing. During imports of large datasets into OpenStreetMap crowdsourcing is used to validate the new data. In humanitarian OpenSteetMap, they map areas during a crisis to support the help organizations through crowdsourcing. 

In the machine learning process, they are starting to use micro-tasks to both validate the created data and also create test data sets to the algorithms. \textit{Interpretation and analysis} tasks rely on the individual to use their interpretation skills during task completion. The task can be to choose between two layers, and decide which is best. This is the task class used in this thesis during the survey. More in section \ref{sec:survey}.

\section{Micro-tasking}\label{sec:microtasking}

%What is it
Micro-tasks should not require any special training and a task should be completed within a couple of minutes \citep{Ipeirotis2010}.  

Micro-tasking and human computation are closely related. In the "Handbook of Human Computation" micro-tasking is present in several sections in the \textit{Human Computation for Disaster Response} chapter [\citep{Meier2013}, p. 95-105].  

\cite{Gadiraju2015} findings when analyzing data from MTurk, indicate rapid growth in micro-task crowdsourcing. Micro-task crowdsourcing refers to a problem-solving model in which a problem or task is outsourced to a distributed group of people by splitting the task or problem into smaller sub-tasks or sub-problems. The sub-tasks or sub-problems are then solved by multiple workers independently, often in return for a reward \citep{Sarasua2012}. 

With the establishment of micro-task crowdsourcing platforms as Amazon's Mechanical Turk (MTurk; www.mturk.com) and CrowdFlower (www.crowdflower.com), micro-tasking is much more accessible. Micro-tasking practitioners are actively turning towards paid crowdsourcing to solve data-centric tasks that require human input \citep{Gadiraju2015}. 

For instance, micro-tasking has already been used to process queries. In the \cite{Franklin2011} paper they extended a traditional query engine with a small number of operations that requires human input by generating and submitting requests to a micro-tasking platform. Most cases of micro-tasking usage exploit the large volume capabilities machines have and the cognitive capabilities of humans \citep{Difallah2016}.  

The task refers to the activity, production or service the company or organization wants to have done.

Problems that are suitable for solving through micro-tasking are those that are easy to distribute into a number of simple tasks, that can be completed in parallel in a relatively short period of time (from seconds to minutes), without requiring specific skills \citep{Sarasua2012}. Research has also demonstrated that micro-tasking is effective for far more complex problems when using sophisticated workflow management techniques. Micro-tasking can then be applied to a broader range of problems like: (1) completing surveys, (2) translating text between two languages, (3) matching pictures of people, (4) summarizing text \citep{Bernstein2015a}, etc. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{../../../../../Desktop/microtasking_illustration}
	\caption{Micro-tasking \citep{Michelucci2015}}
	\label{fig:microtaskingillustration}
\end{figure}

Thanks to micro-tasking platforms as Amazon's Mechanical Turk (MTurk), it is possible to build hybrid human-machine system that combines the scalability of computers with the yet unmatched cognitive abilities of the human brain \citep{Difallah2016}.

In machine-learning algorithms, combining human computation abilities with fast machine learning algorithms has been a success. The team "Tomnod" \footnote{Tomnod is a team of volunteers who work together to identify important objects and interesting places in satellite images; www.tomnod.com} did a project in Australia where they combined human resources and machine learning to locate swimming pools. The machine learning algorithm found polygons where there was likely to be a swimming pool. Then the users participating in finding the swimming pools where only shown polygons the algorithm had selected, minimizing the search area, shown in figure \ref{fig:swimmingpoolpolygon3}. This approach was used in order to reduce the number of user votes required to classify the pools but yet obtain a sufficient confidence \citep{Kostas2016}. This is a good example of micro-tasking. Instead of serving the users with satellite photos of huge areas, the photos was divided into polygons created by an algorithm. They then divided the task into smaller tasks, they micro-tasked the work. 

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{../../../../../Desktop/swimmingpoolpolygon3}
	\caption{Polygons with swimming pools created by the machine learning algorithm \citep{Nikki2016}}
	\label{fig:swimmingpoolpolygon3}
\end{figure}

One of the advantages of micro-tasking platforms like "Tomnod" and "CrowdFlower", mentioned by \cite{Meier2013} (p. 99), is the built-in quality control mechanisms that ensure a relatively high quality of output data. They set a review constraint, for instance in a project where they tagget satellite imagery of Somalia each unique image was reviewed by at least three different volunteers and only when all three agreed on type and location it was approved . 

\subsection{Micro-tasking platforms}
\subsubsection[MTurk]{Amazon's Mechanical Turk}
Amazon's Mechanical Turk (MTurk) is one of the biggest (if not the biggest) micro-tasking platform today. It provides the infrastructure, connectivity and payment mechanisms so that hundreds of thousands of people can perform micro-tasks on the Internet and get paid for it. MTurk is used for many different tasks that are easier for people than computers. In contains simple tasks such as labeling or segmenting images or tagging content, to more complex tasks such as translating or even edition text (\ref{sec:soylent}) \citep{Franklin2011}.  

\subsubsection{Soylent}\label{sec:soylent}
Is a word processing interface that enables writers to call on Mechanical Turk workers to shorten, proofread, and edit parts of their document on demand. To improve the quality of the work, the Soylen team introduced the Find-Fix-Verify crowd programming pattern. This architecture splits tasks into a series of generating and review stages \citep{Bernstein2015a}. 

\subsubsection{Tasking manager}
\subsubsection{Crowdflower}
\subsubsection{CrowdMap}

CrowdMap is implemented using CrowdFlower. 
Is an approach to integrate human and computational intelligence in ontology \footnote{\label{ontology} Ontology is a formal naming and definition of types, properties, and interrelationships of the entities that really or fundamentally exists for a particular domain of discourse. Ontologies are created to limit complexity and to organize information. The ontology can then be applied to problem solving. } alignment tasks via microtask crowdsourcing \citep{Sarasua2012}. Ontology is still (2012) one of those problems that we cannot automate completely and having a human in the loop might increase the quality of the results of machine-driven approaches. 





\subsection{Usage}
A machine learning company called "developmentSEED" use a micro-tasking solution for cleaning machine learning output data. They have created a GUI web application solution called Skynet Scrubber. In their blog, Derek Lieu writes: "Skynet gets more capable every day, but the output is still not perfect [..] We built Skynet Scrub so we could start using Skynet data sooner". 

People are good at comparing items, such as how well an image represents a particular concept. We are also good at finding relevant information with the help of search engines etc \citep{Franklin2011}. By utilizing these qualities in humans, like they did in \citep{Franklin2011} paper by developing a micro-tasking based implementations of query operations, a huge cost and time sparing potential can be utilized. 

\subsection{Challenges}
Getting people to use the micro-tasking platforms. Most of the platforms mentioned in this chapter give payments to the workers. Another options is to make the platform as a game, which is also shown in this chapter. Creating a micro-tasking platform without payments og gamification factors the page is likely to have a short life, even though the tasking manager, supported by HOT, is an exception to this rule. 

When aiming towards wider adoption of crowdsourcing one have to be aware of the challenges of using it. It is important to remember that all tasks do not fit into the micro-tasking crowdworker model. Very complex tasks that can't be partitioned are not suitable for solving through micro-tasks. 

It is important that operations added to a micro-tasking platform considers the talents and limitations of human workers \citep{Franklin2011} and this is what this thesis try to examine. What is the limitations of human workers when dealing with geospatial data. It has been shown that crowds can be "programmed" to execute classical algorithms such as Quicksort, but such use of available resources is neither performant nor cost-efficient \citep{Franklin2011}. 

One problem is that machines can do their operations in real-time, while humans are unpredictable, they can come and go as they wish. This creates a gap where the micro-tasking platforms cannot guarantee on the task completion time \citep{Difallah2016}. 

During the classification of swimming pools in Australia the Tomnod team faced some unexpected challenges. As described in section \ref{sec:microtasking}, they used the crowd to classify if a polygon contained a swimming pool or not, an algorithm had pointed out the polygons first. When reviewing a random sample from the result, they found an indication that 26\% of polygons that contained a pool were identified as not containing pools \citep{Kostas2016}.  Further studies also showed that the guilty part was the crowd, the algorithm had correctly detected polygons containing pools. In a case where the algorithm was 85\% confident that the polygon contained a pool, only one voted 'yes', six voted 'no'. The solution was to combine the human verdict with the machine's prediction. This example show that it is important to use the right combination of humans and machines. Tasks that at first seems simple to do for humans, may be more challenging that expected.  Basic object detection using machine learning perform very well when used together with human operations. 

Advanced computing techniques such as Artificial Intelligence and Machine Learning is needed to build approaches that combine the power of people with the speed and scalability of automated algorithms \citep{Meier2013}. 
