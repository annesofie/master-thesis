Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Tong,
abstract = {—With the rapid development of smartphones, spatial crowdsourcing platforms are getting popular. A foundational research of spatial crowdsourcing is to allocate micro-tasks to suitable crowd workers. Most existing studies focus on offline scenarios, where all the spatiotemporal information of micro-tasks and crowd workers is given. However, they are impractical since micro-tasks and crowd workers in real applications appear dynamically and their spatiotemporal information cannot be known in advance. In this paper, to address the shortcomings of existing offline approaches, we first identify a more practical micro-task allocation problem, called the Global Online Micro-task Allocation in spatial crowdsourcing (GOM A) problem. We first extend the state-of-art algorithm for the online maximum weighted bipartite matching problem to the GOMA problem as the baseline algorithm. Although the baseline algorithm provides theoretical guarantee for the worst case, its average performance in practice is not good enough since the worst case happens with a very low probability in real world. Thus, we consider the average performance of online algorithms, a.k.a online random order model. We propose a two-phase-based framework, based on which we present the TGOA algorithm with 1 4 -competitive ratio under the online random order model. To improve its efficiency, we further design the TGOA-Greedy algorithm following the framework, which runs faster than the TGOA algorithm but has lower competitive ratio of 1 8 . Finally, we verify the effective-ness and efficiency of the proposed methods through extensive experiments on real and synthetic datasets.},
annote = {how to allocate the tasks to suitable workers in real-time dynamic environments (a.k.a online scenarios) and model such online scenarios?

The worker can only conduct tasks that locate within the range, which is shown as a dotted circle in Fig. 1.},
author = {Tong, Yongxin and She, Jieying and Ding, Bolin and Wang, Libin and Chen, Lei},
file = {:Users/AnneSofie/Library/Application Support/Mendeley Desktop/Downloaded/Tong et al. - Unknown - Online Mobile Micro-Task Allocation in Spatial Crowdsourcing.pdf:pdf},
title = {{Online Mobile Micro-Task Allocation in Spatial Crowdsourcing}}
}
@article{Mcandrew2016,
annote = {OSM database schema - apidb.

OSMOSIS tool - perform data management tasks

pg{\_}snapshot - database schema},
author = {Mcandrew, James},
file = {:Users/AnneSofie/Library/Application Support/Mendeley Desktop/Downloaded/Mcandrew - 2016 - Digital Commons @ DU Merging Volunteered Geographic Information Systems.pdf:pdf},
title = {{Digital Commons @ DU Merging Volunteered Geographic Information Systems}},
url = {http://digitalcommons.du.edu/geog{\_}ms{\_}capstone http://digitalcommons.du.edu/geog{\_}ms{\_}capstone/54},
year = {2016}
}
@article{Kittur2008,
abstract = {User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.},
annote = {Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.},
author = {Kittur, Aniket and Chi, Ed H and Suh, Bongwon},
file = {:Users/AnneSofie/Library/Application Support/Mendeley Desktop/Downloaded/Kittur, Chi, Suh - Unknown - Crowdsourcing User Studies With Mechanical Turk.pdf:pdf},
keywords = {Author's kit,Conference Publications,Guides,instructions},
title = {{Crowdsourcing User Studies With Mechanical Turk}},
year = {2008}
}
@article{Leppink2014,
abstract = {In two studies, we investigated whether a recently developed psychometric instrument can differentiate intrinsic, extraneous, and germane cognitive load. Study I revealed a similar three-factor solution for language learning (n = 108) and a statistics lecture (n = 174), and statistics exam scores correlated negatively with the factors assumed to represent intrinsic and extraneous cognitive load during the lecture. In Study II, university freshmen who studied applications of Bayes' theorem in example–example (n = 18) or example–problem (n = 18) condition demonstrated better posttest performance than their peers who studied the applications in problem–example (n = 18) or problem–problem (n = 20) condition, and a slightly modified version of the aforementioned psychometric instrument could help researchers to differentiate intrinsic and extraneous cognitive load. The findings provide support for a recent reconceptualization of germane cognitive load as referring to the actual working memory resources devoted to dealing with intrinsic cognitive load.},
author = {Leppink, Jimmie and Paas, Fred and van Gog, Tamara and van der Vleuten, Cees P.M. and van Merri{\"{e}}nboer, Jeroen J.G.},
doi = {10.1016/j.learninstruc.2013.12.001},
file = {:Users/AnneSofie/Library/Application Support/Mendeley Desktop/Downloaded/Leppink et al. - 2014 - Effects of pairs of problems and examples on task performance and different types of cognitive load.pdf:pdf},
issn = {09594752},
journal = {Learning and Instruction},
pages = {32--42},
title = {{Effects of pairs of problems and examples on task performance and different types of cognitive load}},
url = {http://www.sciencedirect.com/science/article/pii/S0959475213000820},
volume = {30},
year = {2014}
}
@article{Saito2014,
abstract = {We propose a framework of micro-tasking that intrinsically supports the development of workers' skills. It aims to help developers of micro-tasking systems add skill development capabilities to their systems with minimal devel-opment costs. This will allow micro-tasking of skill-intensive work and im-prove the sustainability of micro-tasking systems. Based on the results of the micro-tasking projects we have carried out, our framework has three core mod-ules: tutorial producer, task dispatcher, and feedback visualizer, which are sup-ported by a back-end skill assessment engine. In closing, we discuss ways to apply the proposed framework to realistic micro-tasking situations.},
annote = {If micro-tasking systems support skill development, they can produce outcomes of higher-quality and make their use more sustainable. The skill de- velopment support will make micro-tasking more suitable for more advanced tasks that call for expert skills},
author = {Saito, Shin and Watanabe, Toshihiro and Kobayashi, Masatomo and Takagi, Hironobu},
file = {:Users/AnneSofie/Library/Application Support/Mendeley Desktop/Downloaded/Saito et al. - 2014 - LNCS 8514 - Skill Development Framework for Micro-Tasking.pdf:pdf},
journal = {LNCS},
keywords = {Crowdsourcing,Gamification,Micro-Tasks,Senior Workforce,Skill Assessment,Skill Develop-ment},
pages = {400--409},
title = {{LNCS 8514 - Skill Development Framework for Micro-Tasking}},
volume = {8514},
year = {2014}
}
@article{Difallah,
annote = {Looks at the evolution of a populair micro-task crowdsourcing platform 

HITs = Human Intelligence Tasks
Focus on paid micro-task crowdcourcing, the crowd is asked to perform short tasks, also known as HITs},
author = {Difallah, D. E. and Catasta, M. and Dermartini, G. and Ipeirotis, P. G. and Cudr{\'{e}}-Mauroux, P.},
file = {:Users/AnneSofie/Library/Application Support/Mendeley Desktop/Downloaded/Difallah et al. - Unknown - The Dynamics of Micro-Task Crowdsourcing.pdf:pdf},
journal = {2015},
title = {{The Dynamics of Micro-Task Crowdsourcing}},
url = {http://www.ipeirotis.com/wp-content/uploads/2015/02/frp1365-difallah.pdf}
}
@book{Ben2009,
author = {Ben, Schneiderman and Plaisant, Catherine},
edition = {Fifth},
isbn = {0-321-60148-3},
publisher = {Pearson},
title = {{Designing the User Interface}},
year = {2009}
}
@misc{Gee,
annote = {- You will often hear programmers talking about complexity reduction but actually what is going on is complexity hiding - also known in the wider world as "chunking". The idea behind chunking is that a human can deal with about seven things at a time. 
- The principle of chunking applied to programming is to build a hierarchy of code each composed of small objects that can be understood in a single look. 
- The point is that seven is a guideline and the actual number depends on all sort of factors but "Keep It Small Stupid" is a much better statement of intent than "Keep It Simple Stupid".
- The real confounding factor is what exactly constitutes an item that the "seven" applies to? Should it be seven characters, seven words, seven lines, seven subroutines, seven objects, seven programs....

The Unit Of Comprehension = grad av forst{\aa}else},
author = {Gee, Sue},
title = {{The Magic Number Seven And The Art Of Programming}},
url = {http://www.i-programmer.info/babbages-bag/621-the-magic-number-seven.html},
urldate = {2017-03-21}
}
@article{Deng2016,
abstract = {Advancements in Internet and digital technologies have enabled a new work form of open sourcing, which we refer to as the crowdsourcing work environment (CSWE). This new form of work has the potential to disrupt and transform the nature of traditional work. However, our understanding of this new work form is still in its incipient stage. To enhance our understanding, this study captures crowdworkers' perceptions to explore the characteristics of the crowdworkers, crowdsourcing jobs, and the crowdwork environment that collectively drive the crowdworkers to participate in open source work. Guided by the job characteristic theory and work value perspectives, we used the revealed causal mapping method to analyze narratives by 55 crowdworkers registered on Amazon Mechanical Turk (MTurk). Our data analysis uncovered nine main constructs, 22 key concepts, and 815 causal-effect linkages surrounding CSWE that could guide our theoretical understanding of this emerging phenomenon. Individual needs and the crowdwork context emerged as the major factors motivating individuals' initial participation in CSWE, but we found crowdsourcing task characteristics (e.g., job autonomy, task variety, task significance, task instruction, and task compensation) and a digitally enabled environment (e.g., system affordance and MTurk governance) to shape crowdworkers' continued participation in CSWE. The findings suggest several promising research streams, including the psychological factors (i.e., personal growth needs and work values) and social outcomes (i.e., empowerment or exploitation of crowdworkers) for examining the psychology and sociology of crowdsourcing work.},
annote = {Focuses on the micro-task CSWE (CSWE = crowdsourcing work environment). 

Worried - micro-task workers get poorly paid

Try to answer following: 
Q1 - Why do individuals participate in micro-task CSWE?
Q2 - Their characteristics, and what drives them

Focus on MTurk},
author = {Deng, Xuefei and Joshi, K D},
file = {:Users/AnneSofie/Library/Application Support/Mendeley Desktop/Downloaded/Deng, Joshi - 2016 - Why Individuals Participate in Micro-task Crowdsourcing Work Environment Revealing Crowdworkers' Perceptions.pdf:pdf},
number = {10},
pages = {648--673},
title = {{Why Individuals Participate in Micro-task Crowdsourcing Work Environment: Revealing Crowdworkers' Perceptions}},
volume = {17},
year = {2016}
}
